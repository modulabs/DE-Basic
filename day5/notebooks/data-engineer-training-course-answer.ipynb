{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 수집된 데이터 탐색\n",
    "## 5-1. 스파크 세션 생성\n",
    "### 5-1-1. 스파크 객체를 생성하는 코드를 작성하고, Shift+Enter 로 스파크 버전을 확인합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "# 코어 스파크 라이브러리를 임포트 합니다\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Data Engineer Training Course\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1-2. 수집된 테이블을 데이터프레임으로 읽고, 스키마 및 데이터 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "user25 = spark.read.parquet(\"user/20201025\")\n",
    "user25.printSchema()\n",
    "user25.show(truncate=False)\n",
    "display(user25)\n",
    "\n",
    "purchase25 = spark.read.parquet(\"purchase/20201025\")\n",
    "purchase25.createOrReplaceTempView(\"purchase25\")\n",
    "purchase25.printSchema()\n",
    "display(purchase25)\n",
    "\n",
    "accesslog = spark.read.option(\"inferSchema\", \"true\").json(\"access/20201025\")\n",
    "accesslog.createOrReplaceTempView(\"accesslog\")\n",
    "access25 = spark.sql(\"select a_id, a_tag, a_time, a_timestamp, a_uid from accesslog\")\n",
    "access25.createOrReplaceTempView(\"access25\")\n",
    "access25.printSchema()\n",
    "display(access25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-2. 수집된 고객, 매출 및 접속 임시 테이블 생성\n",
    "### 5-2-1. 데이터프레임을 이용하여 임시테이블 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "user25.createOrReplaceTempView(\"user25\")\n",
    "purchase25.createOrReplaceTempView(\"purchase25\")\n",
    "access25.createOrReplaceTempView(\"access25\")\n",
    "spark.sql(\"show tables '*25'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-3. SparkSQL을 이용하여 테이블 별 데이터프레임 생성하기\n",
    "### 5-3-1. 아래에 비어있는 조건을 채워서 올바른 코드를 작성하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "u_signup_condition = \"u_signup >= '20201025' and u_signup < '20201026'\"\n",
    "user = spark.sql(\"select u_id, u_name, u_gender from user25\").where(u_signup_condition)\n",
    "user.createOrReplaceTempView(\"user\")\n",
    "display(user)\n",
    "\n",
    "p_time_condition = \"p_time >= '2020-10-25 00:00:00' and p_time < '2020-10-26 00:00:00'\"\n",
    "purchase = spark.sql(\"select from_unixtime(p_time) as p_time, p_uid, p_id, p_name, p_amount from purchase25\").where(p_time_condition)\n",
    "purchase.createOrReplaceTempView(\"purchase\")\n",
    "display(purchase)\n",
    "\n",
    "access = spark.sql(\"select a_id, a_tag, a_timestamp, a_uid from access25\")\n",
    "access.createOrReplaceTempView(\"access\")\n",
    "display(access)\n",
    "\n",
    "spark.sql(\"show tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-4. 생성된 테이블을 SQL 문을 이용하여 탐색하기\n",
    "### 5-4-1. 한 쪽의 성별('남' 혹은 '여')을 가진 목록을 출력하세요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "spark.sql(\"describe user\")\n",
    "spark.sql(\"select * from user\")\n",
    "whereCondition = \"u_gender = '남'\"\n",
    "spark.sql(\"select * from user\").where(whereCondition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-4-2. 상품금액이 200만원을 초과하는 매출 목록을 출력하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "spark.sql(\"describe purchase\")\n",
    "spark.sql(\"select * from purchase\")\n",
    "selectClause = \"select * from purchase where p_amount > 2000000\"\n",
    "spark.sql(selectClause)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-4-3. GroupBy 구문을 이용하여 로그인, 로그아웃 횟수를 출력하세요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "spark.sql(\"describe access\")\n",
    "spark.sql(\"select * from access\")\n",
    "groupByClause=\"select a_id, count(a_id) from access group by a_id\"\n",
    "spark.sql(groupByClause)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 기본 지표 생성\n",
    "## 6-1. DAU (Daily Activer User) 지표를 생성하세요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "display(access)\n",
    "distinctAccessUser = \"select count(distinct a_uid) as DAU from access\"\n",
    "dau = spark.sql(distinctAccessUser)\n",
    "display(dau)\n",
    "v_dau = dau.collect()[0][\"DAU\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-2. DPU (Daily Paying User) 지표를 생성하세요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "\n",
    "display(purchase)\n",
    "distinctPayingUser = \"select count(distinct p_uid) as PU from purchase\"\n",
    "pu = spark.sql(distinctPayingUser)\n",
    "display(pu)\n",
    "v_pu = pu.collect()[0][\"PU\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-3. DR (Daily Revenue) 지표를 생성하세요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "display(purchase)\n",
    "sumOfDailyRevenue = \"select sum(p_amount) as DR from purchase\"\n",
    "dr = spark.sql(sumOfDailyRevenue)\n",
    "display(dr)\n",
    "v_dr = dr.collect()[0][\"DR\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-4. ARPU (Average Revenue Per User) 지표를 생성하세요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "# print(\"ARPU : {}\".format(<유저당 매출 금액 계산식>))\n",
    "print(\"+------------------+\")\n",
    "print(\"|             ARPU |\")\n",
    "print(\"+------------------+\")\n",
    "print(\"|        {} |\".format(v_dr / v_dau))\n",
    "print(\"+------------------+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-5. ARPPU (Average Revenue Per Paying User) 지표를 생성하세요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "print(\"+------------------+\")\n",
    "print(\"|            ARPPU |\")\n",
    "print(\"+------------------+\")\n",
    "print(\"|        {} |\".format(v_dr / v_pu))\n",
    "print(\"+------------------+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 고급 지표 생성\n",
    "## 7-1. 디멘젼 테이블을 설계 합니다\n",
    "## 7-2. 오픈 첫 날 접속한 모든 고객 및 접속 횟수를 가진 데이터프레임을 생성합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "access.printSchema()\n",
    "countOfAccess = \"select a_uid, count(a_uid) as a_count from access where a_id = 'login' group by a_uid\"\n",
    "accs = spark.sql(countOfAccess)\n",
    "display(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-3. 일 별 이용자 별 총 매출 금액과, 구매 횟수를 가지는 데이터프레임을 생성합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "purchase.printSchema()\n",
    "sumOfCountAndAmount = \"select p_uid, sum(p_amount) as p_amount, count(p_uid) as p_count from purchase group by p_uid\"\n",
    "amts = spark.sql(sumOfCountAndAmount)\n",
    "display(amts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-4. 이용자 정보와 구매 정보와 조인합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "accs.printSchema()\n",
    "amts.printSchema()\n",
    "joinCondition = accs[\"a_uid\"] == amts[\"p_uid\"]\n",
    "joinHow = \"left_outer\"\n",
    "dim1 = accs.join(amts, joinCondition, joinHow)\n",
    "dim1.printSchema()\n",
    "display(dim1.orderBy(asc(\"a_uid\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-5. 고객 정보를 추가합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "dim1.printSchema()\n",
    "user.printSchema()\n",
    "joinCondition = dim1[\"a_uid\"] == user[\"u_id\"]\n",
    "joinHow = \"left_outer\"\n",
    "dim2 = dim1.join(user, joinCondition, joinHow)\n",
    "dim2.printSchema()\n",
    "display(dim2.orderBy(asc(\"a_uid\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-6. 중복되는 ID 컬럼은 제거하고, 숫자 필드에 널값은 0으로 기본값을 넣어줍니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "dim2.printSchema()\n",
    "dim3 = dim2.drop(\"p_uid\", \"u_id\")\n",
    "fillDefaultValue = {\"p_amount\":0, \"p_count\":0}\n",
    "dim4 = dim3.na.fill(fillDefaultValue)\n",
    "dim4.printSchema()\n",
    "display(dim4.orderBy(asc(\"a_uid\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-7. 생성된 유저 테이블을 재사용 가능하도록 컬럼 명을 변경합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "dim4.printSchema()\n",
    "dim5 = (\n",
    "    dim4\n",
    "    .withColumnRenamed(\"a_uid\", \"d_uid\")\n",
    "    .withColumnRenamed(\"a_count\", \"d_acount\")\n",
    "    .withColumnRenamed(\"p_amount\", \"d_pamount\")\n",
    "    .withColumnRenamed(\"p_count\", \"d_pcount\")\n",
    "    .withColumnRenamed(\"u_name\", \"d_name\")\n",
    "    .withColumnRenamed(\"u_gender\", \"d_gender\")\n",
    "   .drop(\"a_uid\", \"a_count\", \"p_amount\", \"p_count\", \"u_name\", \"u_gender\")\n",
    "   .select(\"d_uid\", \"d_name\", \"d_gender\", \"d_acount\", \"d_pamount\", \"d_pcount\")\n",
    ")\n",
    "display(dim5.orderBy(asc(\"d_uid\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-8. 최초 구매 유저 정보를 추가합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "purchase.printSchema()\n",
    "selectFirstPurchaseTime = \"select p_uid, min(p_time) as p_time from purchase group by p_uid\"\n",
    "\n",
    "first_purchase = spark.sql(selectFirstPurchaseTime)\n",
    "dim6 = dim5.withColumn(\"d_first_purchase\", lit(None))\n",
    "dim6.printSchema()\n",
    "\n",
    "exprFirstPurchase = expr(\"case when d_first_purchase is null then p_time else d_first_purchase end\")\n",
    "\n",
    "dim7 = (\n",
    "    dim6.join(first_purchase, dim5.d_uid == first_purchase.p_uid, \"left_outer\")\n",
    "    .withColumn(\"first_purchase\", exprFirstPurchase)\n",
    "    .drop(\"d_first_purchase\", \"p_uid\", \"p_time\")\n",
    "    .withColumnRenamed(\"first_purchase\", \"d_first_purchase\")\n",
    ")\n",
    "    \n",
    "dimension = dim7.orderBy(asc(\"d_uid\"))\n",
    "dimension.printSchema()\n",
    "display(dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-9. 생성된 디멘젼을 저장소에 저장합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "dimension.printSchema()\n",
    "target_dir=\"dim_users/dt=20201025\"\n",
    "dimension.write.mode(\"overwrite\").parquet(target_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-10. 생성된 디멘젼을 다시 읽어서 출력합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "newDimension = spark.read.parquet(target_dir)\n",
    "newDimension.printSchema()\n",
    "display(newDimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 결과가 아래의 값과 일치하는지 확인합니다\n",
    "# DAU:5, PU:4, DR: 12200000, ARPU: 2440000.0, ARPPU: 3050000.0\n",
    "print(\"+------------------+\")\n",
    "print(\"|              DAU |\")\n",
    "print(\"+------------------+\")\n",
    "print(\"|                {} |\".format(v_dau))\n",
    "print(\"+------------------+\")\n",
    "print(\"+------------------+\")\n",
    "print(\"|               PU |\")\n",
    "print(\"+------------------+\")\n",
    "print(\"|                {} |\".format(v_pu))\n",
    "print(\"+------------------+\")\n",
    "print(\"+------------------+\")\n",
    "print(\"|               DR |\")\n",
    "print(\"+------------------+\")\n",
    "print(\"|         {} |\".format(v_dr))\n",
    "print(\"+------------------+\")\n",
    "print(\"+------------------+\")\n",
    "print(\"|             ARPU |\")\n",
    "print(\"+------------------+\")\n",
    "print(\"|        {} |\".format(v_dr / v_dau))\n",
    "print(\"+------------------+\")\n",
    "print(\"+------------------+\")\n",
    "print(\"|            ARPPU |\")\n",
    "print(\"+------------------+\")\n",
    "print(\"|        {} |\".format(v_dr / v_pu))\n",
    "print(\"+------------------+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
