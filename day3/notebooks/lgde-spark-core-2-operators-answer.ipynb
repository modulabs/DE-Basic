{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2교시 기본 연산자 다루기\n",
    "\n",
    "> 스파크의 \"기본 연산자\" 와 \"데이터프레임\"에 대해 학습합니다\n",
    "\n",
    "## 목차\n",
    "* [1. 기본 연산자](#1.-기본-연산자)\n",
    "  - [1.1 데이터 프레임 함수](#1.1-데이터-프레임-x함수)\n",
    "  - [1.2 컬럼 함수](#1.2-컬럼-함수)\n",
    "  - [1.3 기타 함수](#1.3-기타-함수)\n",
    "* [2. RDD 의 특징](#3.-RDD-의-특징)\n",
    "  - [2.1 RDD 통한 데이터 변환](#3.1-RDD-통한-데이터-변환)\n",
    "  - [2.2 구조화 API 통한 데이터 변환](#3.2-구조화-API-통한-데이터-변환)\n",
    "* [3. 데이터 타입](#4.-데이터-타입)\n",
    "* [4. 핵심 데이터 프레임 연산자](#2.-핵심-데이터-프레임-연산자)\n",
    "  - [4.1 파일로 부터 테이블 만들어 사용하기](#2.1-파일로-부터-테이블-만들어-사용하기)\n",
    "  - [4.2 특정 컬럼 선택 (select, selectExpr)](#2.2-특정-컬럼-선택-(select,-selectExpr))\n",
    "  - [4.3 상수값 사용하기](#2.3-상수값-사용하기)\n",
    "  - [4.4 컬럼 추가하기](#2.4-컬럼-추가하기)\n",
    "  - [4.5 컬럼명 바꾸기](#2.5-컬럼명-바꾸기)\n",
    "  - [4.6 컬럼 제거하기](#2.6-컬럼-제거하기)\n",
    "  - [4.7 컬럼의 데이터 타입 변경하기](#2.7-컬럼의-데이터-타입-변경하기)\n",
    "  - [4.8 레코드 필터링](#2.8-레코드-필터링)\n",
    "  - [4.9 유일값 (DISTINCT)](#2-9-유일값-(DISTINCT))\n",
    "  - [4.10 정렬 (SORT)](#2.10-정렬-(SORT))\n",
    "  - [4.11 로우 수 제한 (LIMIT)](#2.11-로우-수-제한-(LIMIT))\n",
    "* [5. 기타 데이터 프레임 연산자](#5.-기타-데이터-프레임-연산자)\n",
    "  - [5.1 사전에 스키마를 정의하는 장점](#5.1-사전에-스키마를-정의하는-장점)\n",
    "  - [5.2 스키마를 정의하는 두 가지 방법](#5.2-스키마를-정의하는-두-가지-방법)\n",
    "  - [5.3 중첩된 배열 스키마](#5.3-중첩된-배열-스키마)\n",
    "  - [5.4 컬럼과 표현식](#5.4-컬럼과-표현식)\n",
    "  - [5.5 로우 생성 및 다루기](#5.5-로우-생성-및-다루기)\n",
    "  - [5.6 파케이 파일 혹은 테이블 저장](#5.6-파케이-파일-혹은-테이블-저장)\n",
    "  - [5.7 프로젝션과 필터](#5.7-프로젝션과-필터)\n",
    "  - [5.8 날짜 관련 함수](#5.8-날짜-관련-함수)\n",
    "* [6. 데이터셋 API](#6.-데이터셋-API)\n",
    "  - [6.1 데이터셋과 데이터프레임 비교](#6.1-데이터셋과-데이터프레임-비교)\n",
    "  - [6.2 데이터셋 데이터프레임 그리고 RDD](#6.2-데이터셋-데이터프레임-그리고-RDD)\n",
    "* [7. 카탈리스트 옵티마이저](#7.-카탈리스트-옵티마이저)\n",
    "  - [7.1 분석 (Analysis)](#7.1-분석-(Analysis))\n",
    "  - [7.2 논리 최적화 (Logical Optimization)](#7.2-논리-최적화-(Logical-Optimization))\n",
    "  - [7.3 물리 계획 (Physical Planning)](#7.3-물리-계획-(Physical-Planning))\n",
    "  - [7.4 코드 생성 (Code Generation)](#7.4-코드-생성-(Code-Generation))\n",
    "* [8. 실습 문제](#8.-실습-문제)\n",
    "* [참고자료](#참고자료)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 기본 연산자\n",
    "---\n",
    "### 1.1 데이터 프레임 함수\n",
    "| 함수 | 설명 | 기타 |\n",
    "| - | - | - |\n",
    "| df.printSchema() | 스키마 정보를 출력합니다. | - |\n",
    "| df.schema | StructType 스키마를 반환합니다 | - |\n",
    "| df.columns | 컬럼명 정보를 반환합니다 | - |\n",
    "| df.show(n) | 데이터 n 개를 출력합니다 | - |\n",
    "| df.first() | 데이터 프레임의 첫 번째 Row 를 반환합니다 | - |\n",
    "| df.head(n) | 데이터 프레임의 처음부터 n 개의 Row 를 반환합니다 | - |\n",
    "| df.createOrReplaceTempView | 임시 뷰 테이블을 생성합니다 | - |\n",
    "| df.union(newdf) | 데이터프레임 간의 유니온 연산을 수행합니다 | - |\n",
    "| df.limit(n) | 추출할 로우수 제한 | T |\n",
    "| df.repartition(n) | 파티션 재분배, 셔플발생 | - |\n",
    "| df.coalesce() | 셔플하지 않고 파티션을 병합 | 마지막 스테이지의 reduce 수가 줄어드는 효과로 성능저하에 유의해야 합니다 |\n",
    "| df.collect() | 모든 데이터 수집, 반환 | A |\n",
    "| df.take(n) | 상위 n개 로우 반환 | A |\n",
    "\n",
    "---\n",
    "### 1.2 컬럼 함수\n",
    "| 함수 | 설명 | 기타 |\n",
    "| - | - | - |\n",
    "| df.select | 컬럼이나 표현식 사용  | - |\n",
    "| df.selectExpr | 문자열 표현식 사용 = df.select(expr()) | - |\n",
    "| df.withColumn(컬럼명, 표현식) | 컬럼 추가, 비교, 컬럼명 변경 | - |\n",
    "| df.withColumnRenamed(old_name, new_name) | 컬럼명 변경 | - |\n",
    "| df.drop() | 컬럼 삭제 | - |\n",
    "| df.where | 로우 필터링 | - |\n",
    "| df.filter | 로우 필터링 | - |\n",
    "| df.sort, df.orderBy | 정렬 | - |\n",
    "| df.sortWithinPartitions | 파티션별 정렬 | - |\n",
    "\n",
    "---\n",
    "### 1.3 기타 함수\n",
    "| 함수 | 설명 | 기타 |\n",
    "| - | - | - |\n",
    "| expr(\"someCol - 5\") | 표현식 | - |\n",
    "| lit() | 리터럴 | - |\n",
    "| cast() | 컬럼 데이터 타입 변경 | - |\n",
    "| distinct() | unique row | - |\n",
    "| desc(), asc() | 정렬 순서 | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RDD 의 특징\n",
    "\n",
    "| 특징 | 설명 | 기타 |\n",
    "|---|---|---|\n",
    "| dependencies | resilency | 리니지를 통해 의존성 정보를 유지함으로써 언제든 다시 수행할 수 있는 회복력을 가집니다 |\n",
    "| partitions | parallelize computation | 파티션 단위로 데이터를 저장 관리하므로써 병렬 처리를 가능하게 합니다 |\n",
    "| compute function | Iterator\\[T\\] | RDD로 저장되는 모든 데이터는 반복자를 통해 함수를 적용할 수 있습니다 |\n",
    "\n",
    "* 반면에 compute function 의 내부를 spark 가 알 수 없기 때문에 오류를 찾아내가 어려우며, Python 과 같은 스크립트 언어는 generic object 로만 인식이 되므로 호환하기 어려우며, T 타입의 객체는 직렬화되어 전달되기만 할 뿐 스파크는 해당 데이터 타입 T 에 대해 알 수 없습니다\n",
    "\n",
    "> RDD 를 통해 데이터 처리하는 방법과, 구조화된 API 를 통해 처리하는 방법을 비교해 보고, 이러한 고수준의 DSL 연산자를 통해 보다 단순하게 표현이 가능합니다.\n",
    "\n",
    "### 2.1 RDD 통한 데이터 변환 (참고용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|  Name| Age|\n",
      "+------+----+\n",
      "|   Cat|27.0|\n",
      "|   Dog|19.0|\n",
      "|Monkey|28.0|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataRDD = spark.sparkContext.parallelize([(\"Cat\", 30), (\"Dog\", 28), (\"Monkey\", 28), (\"Cat\", 24), (\"Dog\", 10)])\n",
    "agesRDD = (\n",
    "    dataRDD.map(lambda x: (x[0], (x[1], 1)))\n",
    "     .reduceByKey(lambda v1, v2: (v1[0] + v2[0], v1[1] + v2[1]))\n",
    "     .map(lambda v: (v[0], v[1][0]/v[1][1]))\n",
    ")\n",
    "agesRDD.toDF([\"Name\", \"Age\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 구조화 API 통한 데이터 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|Name  |Age |\n",
      "+------+----+\n",
      "|Cat   |27.0|\n",
      "|Monkey|28.0|\n",
      "|Dog   |19.0|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"동물의 평균 수명\").getOrCreate()\n",
    "animal = spark.createDataFrame([(\"Cat\", 30), (\"Dog\", 28), (\"Monkey\", 28), (\"Cat\", 24), (\"Dog\", 10)], [\"Name\", \"Age\"])\n",
    "ages = animal.select(\"Name\", \"Age\").groupBy(\"Name\").agg(avg(\"Age\").alias(\"Age\"))\n",
    "ages.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 타입\n",
    "> immutable 하며, 모든 transformation 들의 lineage 를 유지합니다. 또한 컬럼을 변경, 추가 등을 통해 새로운 데이터프레임을 생성합니다.\n",
    "\n",
    "| python | scala |\n",
    "|---|---|\n",
    "| ![python](images/datatypes-python.png) | ![scala](images/datatypes-scala.png) | \n",
    "| ![python](images/datatypes-python2.png) | ![scala](images/datatypes-scala2.png) | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 핵심 데이터 프레임 연산자\n",
    "\n",
    "### 4.1 파일로 부터 테이블 만들어 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 원시 데이터로 부터 읽거나, Spark SQL 통한 결과는 항상 데이터프레임이 생성됩니다\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 원시 데이터로 부터 읽거나, Spark SQL 통한 결과는 항상 데이터프레임이 생성됩니다\")\n",
    "df = spark.read.json(\"data/flight-data/json/2015-summary.json\")\n",
    "df.createOrReplaceTempView(\"2015_summary\")\n",
    "\n",
    "sql_result = spark.sql(\"SELECT * FROM 2015_summary\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 특정 컬럼 선택 (select, selectExpr)\n",
    "> 아래의 모든 예제에서 컬럼 선택 시에 select(col(\"컬럼명\")) 으로 접근할 수도 있지만 **selectExpr(\"컬럼명\") 이 간결하기 때문에 앞으로는 가능한 표현식으로 사용**하겠습니다 <br>\n",
    "컬럼 표현식의 경우 반드시 하나의 컬럼은 하나씩 표현되어야만 합니다.  <br>\n",
    "잘된예 : \"컬럼1\", \"컬럼2\" <br>\n",
    "잘못된예: \"컬럼1, 컬럼2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# select 표현은 컬럼만 입력이 가능하며, 함수나 기타 표현식을 사용할 수 없습니다. 사용하기 위해서는 functions 를 임포트 하고, 개별 함수의 특징을 잘 이해하고 사용해야 합니다\n",
      "+------------------------+-------------------+\n",
      "|upper(DEST_COUNTRY_NAME)|ORIGIN_COUNTRY_NAME|\n",
      "+------------------------+-------------------+\n",
      "|           UNITED STATES|            Romania|\n",
      "|           UNITED STATES|            Croatia|\n",
      "+------------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "# selectExpr 별도의 임포트 없이, 모든 표현식을 사용할 수 있습니다\n",
      "+------------------------+-------------------+\n",
      "|upper(DEST_COUNTRY_NAME)|ORIGIN_COUNTRY_NAME|\n",
      "+------------------------+-------------------+\n",
      "|           UNITED STATES|            Romania|\n",
      "|           UNITED STATES|            Croatia|\n",
      "+------------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "print(\"# select 표현은 컬럼만 입력이 가능하며, 함수나 기타 표현식을 사용할 수 없습니다. 사용하기 위해서는 functions 를 임포트 하고, 개별 함수의 특징을 잘 이해하고 사용해야 합니다\")\n",
    "df.select(upper(col(\"DEST_COUNTRY_NAME\")), \"ORIGIN_COUNTRY_NAME\").show(2)\n",
    "\n",
    "print(\"# selectExpr 별도의 임포트 없이, 모든 표현식을 사용할 수 있습니다\")\n",
    "df.selectExpr(\"upper(DEST_COUNTRY_NAME)\", \"ORIGIN_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 컬럼의 앨리어스 혹은 전체 컬럼을 위한 * 도 사용할 수 있습니다\n",
      "+-------------+-----------------+\n",
      "| newColmnName|DEST_COUNTRY_NAME|\n",
      "+-------------+-----------------+\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "+-------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 컬럼의 앨리어스 혹은 전체 컬럼을 위한 * 도 사용할 수 있습니다\")\n",
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColmnName\", \"DEST_COUNTRY_NAME\").show(2)\n",
    "\n",
    "df.selectExpr(\"*\", \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>1. [기본]</font> \"data/flight-data/json/2015-summary.json\" 경로의 JSON 데이터를 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 10건의 데이터를 출력하세요\n",
    "#### 3. selectExpr 구문 혹은 spark sql 구문을 이용하여 DEST_COUNTRY_NAME 컬럼은 대문자로, ORIGIN_COUNTRY_NAME 컬럼을 소문자로 2개의 컬럼을 조회하세요\n",
    "\n",
    "<details><summary>[실습1] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "df1 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(\"data/flight-data/json/2015-summary.json\")\n",
    ")\n",
    "    \n",
    "df1.printSchema()\n",
    "answer = df1.createOrReplaceTempView(\"2015_summary\")\n",
    "spark.sql(\"select upper(DEST_COUNTRY_NAME), lower(ORIGIN_COUNTRY_NAME) from 2015_summary\").show(10)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "+------------------------+--------------------------+\n",
      "|upper(DEST_COUNTRY_NAME)|lower(ORIGIN_COUNTRY_NAME)|\n",
      "+------------------------+--------------------------+\n",
      "|           UNITED STATES|                   romania|\n",
      "|           UNITED STATES|                   croatia|\n",
      "|           UNITED STATES|                   ireland|\n",
      "|                   EGYPT|             united states|\n",
      "|           UNITED STATES|                     india|\n",
      "|           UNITED STATES|                 singapore|\n",
      "|           UNITED STATES|                   grenada|\n",
      "|              COSTA RICA|             united states|\n",
      "|                 SENEGAL|             united states|\n",
      "|                 MOLDOVA|             united states|\n",
      "+------------------------+--------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df1 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(\"data/flight-data/json/2015-summary.json\")\n",
    ")\n",
    "\n",
    "df1.printSchema()\n",
    "answer = df1.createOrReplaceTempView(\"2015_summary\")\n",
    "spark.sql(\"select upper(DEST_COUNTRY_NAME), lower(ORIGIN_COUNTRY_NAME) from 2015_summary\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 상수값 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 리터럴(literal)을 사용한 리터럴 상수 값 컬럼 추가\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)\n",
    "df.selectExpr(\"*\", \"1 as One\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 컬럼 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# withColumn(컬럼명, 표현식) 으로 컬럼 추가\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# withColumn(컬럼명, 표현식) 으로 컬럼 추가\")\n",
    "df.withColumn(\"numberOne\", lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 컬럼의 대소 비교를 통한 불리언 값 반환\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 컬럼의 대소 비교를 통한 불리언 값 반환\")\n",
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 존재하는 컬럼을 표현식을 통해 새로운 컬럼 생성, 기존 컬럼을 삭제\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- Destination: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 존재하는 컬럼을 표현식을 통해 새로운 컬럼 생성, 기존 컬럼을 삭제\")\n",
    "before = df\n",
    "before.printSchema()\n",
    "\n",
    "after = before.withColumn(\"Destination\", expr(\"DEST_COUNTRY_NAME\"))\n",
    "after.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>2. [기본]</font> \"data/flight-data/json/2015-summary.json\" 경로의 JSON 데이터를 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 10건의 데이터를 출력하세요\n",
    "#### 3. 기존의 컬럼은 그대로 두고, ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME 2개의 컬럼을 각각 소문자, 대문자로 변경한 컬럼 ORIGIN_COUNTRY_NAME_LOWER, DEST_COUNTRY_NAME_UPPER 총 4개의 컬럼을 출력하세요\n",
    "\n",
    "<details><summary>[실습2] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "df2 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(\"data/flight-data/json/2015-summary.json\")\n",
    ")\n",
    "    \n",
    "df2.printSchema()\n",
    "answer = df2.createOrReplaceTempView(\"2015_summary\")\n",
    "spark.sql(\"\"\"select \n",
    "    ORIGIN_COUNTRY_NAME, \n",
    "    DEST_COUNTRY_NAME, \n",
    "    lower(ORIGIN_COUNTRY_NAME) as ORIGIN_COUNTRY_NAME_LOWER, \n",
    "    upper(DEST_COUNTRY_NAME) as DEST_COUNTRY_NAME_UPPER \n",
    "    from 2015_summary\"\"\"\n",
    ").show(10)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "+-------------------+-----------------+-------------------------+-----------------------+\n",
      "|ORIGIN_COUNTRY_NAME|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME_LOWER|DEST_COUNTRY_NAME_UPPER|\n",
      "+-------------------+-----------------+-------------------------+-----------------------+\n",
      "|            Romania|    United States|                  romania|          UNITED STATES|\n",
      "|            Croatia|    United States|                  croatia|          UNITED STATES|\n",
      "|            Ireland|    United States|                  ireland|          UNITED STATES|\n",
      "|      United States|            Egypt|            united states|                  EGYPT|\n",
      "|              India|    United States|                    india|          UNITED STATES|\n",
      "|          Singapore|    United States|                singapore|          UNITED STATES|\n",
      "|            Grenada|    United States|                  grenada|          UNITED STATES|\n",
      "|      United States|       Costa Rica|            united states|             COSTA RICA|\n",
      "|      United States|          Senegal|            united states|                SENEGAL|\n",
      "|      United States|          Moldova|            united states|                MOLDOVA|\n",
      "+-------------------+-----------------+-------------------------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df2 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(\"data/flight-data/json/2015-summary.json\")\n",
    ")\n",
    "\n",
    "df2.printSchema()\n",
    "answer = df2.createOrReplaceTempView(\"2015_summary\")\n",
    "spark.sql(\"\"\"select \n",
    "    ORIGIN_COUNTRY_NAME, \n",
    "    DEST_COUNTRY_NAME, \n",
    "    lower(ORIGIN_COUNTRY_NAME) as ORIGIN_COUNTRY_NAME_LOWER, \n",
    "    upper(DEST_COUNTRY_NAME) as DEST_COUNTRY_NAME_UPPER \n",
    "    from 2015_summary\"\"\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 컬럼명 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 컬럼 명 변경하기\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Destination', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# 컬럼 명 변경하기\")\n",
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"Destination\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 컬럼 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 특정 컬럼을 제거합니다\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# 특정 컬럼을 제거합니다\")\n",
    "df.printSchema()\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 기본적으로 스파크는 대소문자를 가리지 않지만, 옵션을 통해서 구분이 가능합니다\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 기본적으로 스파크는 대소문자를 가리지 않지만, 옵션을 통해서 구분이 가능합니다\")\n",
    "spark.conf.set('spark.sql.caseSensitive', True)\n",
    "caseSensitive = df.drop(\"dest_country_name\")\n",
    "caseSensitive.printSchema()\n",
    "\n",
    "spark.conf.set('spark.sql.caseSensitive', False)\n",
    "caseInsensitive = df.drop(\"dest_country_name\")\n",
    "caseInsensitive.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 한 번에 여러 컬럼도 삭제할 수 있습니다\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['count']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"# 한 번에 여러 컬럼도 삭제할 수 있습니다\")\n",
    "df.printSchema()\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").columns # 여러 컬럼을 지우기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>3. [중급]</font> \"data/flight-data/json/2015-summary.json\" 경로의 JSON 데이터를 읽고\n",
    "#### 1. ORIGIN_COUNTRY_NAME 컬럼은 Origin 으로 이름을 변경하고\n",
    "#### 2. DEST_COUNTRY_NAME 컬럼은 DestUpper 으로 대문자로 변경한 컬럼을 추가하고\n",
    "#### 3. DEST_COUNTRY_NAME 컬럼은 Drop 하고, Origin, DestUpper 2개의 컬럼만 남은 DataFrame 의 데이터를 출력하세요\n",
    "#### 4. 최종 스키마를 출력하세요\n",
    "\n",
    "<details><summary>[실습3] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "df3 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(\"data/flight-data/json/2015-summary.json\")\n",
    ")\n",
    "    \n",
    "df3.printSchema()\n",
    "answer = (df3\n",
    "    .withColumnRenamed(\"ORIGIN_COUNTRY_NAME\", \"Origin\")\n",
    "    .withColumn(\"DestUpper\", upper(\"DEST_COUNTRY_NAME\"))\n",
    "    .drop(\"DEST_COUNTRY_NAME\", \"count\")\n",
    ")\n",
    "answer.show()\n",
    "answer.printSchema()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "+----------------+--------------------+\n",
      "|          Origin|           DestUpper|\n",
      "+----------------+--------------------+\n",
      "|         Romania|       UNITED STATES|\n",
      "|         Croatia|       UNITED STATES|\n",
      "|         Ireland|       UNITED STATES|\n",
      "|   United States|               EGYPT|\n",
      "|           India|       UNITED STATES|\n",
      "|       Singapore|       UNITED STATES|\n",
      "|         Grenada|       UNITED STATES|\n",
      "|   United States|          COSTA RICA|\n",
      "|   United States|             SENEGAL|\n",
      "|   United States|             MOLDOVA|\n",
      "|    Sint Maarten|       UNITED STATES|\n",
      "|Marshall Islands|       UNITED STATES|\n",
      "|   United States|              GUYANA|\n",
      "|   United States|               MALTA|\n",
      "|   United States|            ANGUILLA|\n",
      "|   United States|             BOLIVIA|\n",
      "|        Paraguay|       UNITED STATES|\n",
      "|   United States|             ALGERIA|\n",
      "|   United States|TURKS AND CAICOS ...|\n",
      "|       Gibraltar|       UNITED STATES|\n",
      "+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- DestUpper: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df3 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(\"data/flight-data/json/2015-summary.json\")\n",
    ")\n",
    "\n",
    "df3.printSchema()\n",
    "answer = (df3\n",
    "    .withColumnRenamed(\"ORIGIN_COUNTRY_NAME\", \"Origin\")\n",
    "    .withColumn(\"DestUpper\", upper(\"DEST_COUNTRY_NAME\"))\n",
    "    .drop(\"DEST_COUNTRY_NAME\", \"count\")\n",
    ")\n",
    "answer.show()\n",
    "answer.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 컬럼의 데이터 타입 변경하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 컬럼의 데이터 유형을 변경합니다\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|str_count|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|       15|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "|    United States|            Ireland|  344|      344|\n",
      "|            Egypt|      United States|   15|       15|\n",
      "|    United States|              India|   62|       62|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- str_count: string (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+---------+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|str_count|int_count|\n",
      "+-----------------+-------------------+-----+---------+---------+\n",
      "|    United States|            Romania|   15|       15|       15|\n",
      "|    United States|            Croatia|    1|        1|        1|\n",
      "|    United States|            Ireland|  344|      344|      344|\n",
      "|            Egypt|      United States|   15|       15|       15|\n",
      "|    United States|              India|   62|       62|       62|\n",
      "+-----------------+-------------------+-----+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- str_count: string (nullable = true)\n",
      " |-- int_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 컬럼의 데이터 유형을 변경합니다\")\n",
    "df.printSchema()\n",
    "\n",
    "int2str = df.withColumn(\"str_count\", col(\"count\").cast(\"string\"))\n",
    "int2str.show(5)\n",
    "int2str.printSchema()\n",
    "\n",
    "str2int = int2str.withColumn(\"int_count\", col(\"str_count\").cast(\"int\"))\n",
    "str2int.show(5)\n",
    "str2int.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 레코드 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Where 와 Filter 는 동일합니다\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "# 같은 표현식에 여러 필터를 적용하는 것도 가능합니다\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# Where 와 Filter 는 동일합니다\")\n",
    "df.where(\"count < 2\").show(2)\n",
    "df.filter(\"count < 2\").show(2)\n",
    "\n",
    "print(\"# 같은 표현식에 여러 필터를 적용하는 것도 가능합니다\")\n",
    "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 유일 값 (DISTINCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "125\n"
     ]
    }
   ],
   "source": [
    "\"\"\" distinct 함수 \"\"\"\n",
    "print(df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count())\n",
    "print(df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count())\n",
    "# distinctcount?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>4. [기본] </font> \"data/flight-data/json/2015-summary.json\" 경로의 JSON 데이터를 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 데이터 10건을 출력하세요\n",
    "#### 3. count 가 5000 이상, 100000 보다 미만인 ORIGIN_COUNTRY_NAME 를 출력하되 중복을 제거해 주세요\n",
    "\n",
    "<details><summary>[실습4] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "df4 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(\"data/flight-data/json/2015-summary.json\")\n",
    ")\n",
    "    \n",
    "df4.printSchema()\n",
    "df4.show(10)\n",
    "df4.selectExpr(\"min(count)\", \"max(count)\").show()\n",
    "answer = df4.where(expr(\"count >= 5000 and count < 100000\")).select(\"ORIGIN_COUNTRY_NAME\")\n",
    "answer.distinct()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------+----------+\n",
      "|min(count)|max(count)|\n",
      "+----------+----------+\n",
      "|         1|    370002|\n",
      "+----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>ORIGIN_COUNTRY_NAME</th></tr>\n",
       "<tr><td>United States</td></tr>\n",
       "<tr><td>Mexico</td></tr>\n",
       "<tr><td>Canada</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------------+\n",
       "|ORIGIN_COUNTRY_NAME|\n",
       "+-------------------+\n",
       "|      United States|\n",
       "|             Mexico|\n",
       "|             Canada|\n",
       "+-------------------+"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df4 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(\"data/flight-data/json/2015-summary.json\")\n",
    ")\n",
    "\n",
    "df4.printSchema()\n",
    "df4.show(10)\n",
    "df4.selectExpr(\"min(count)\", \"max(count)\").show()\n",
    "answer = df4.where(expr(\"count >= 5000 and count < 100000\")).select(\"ORIGIN_COUNTRY_NAME\")\n",
    "answer.distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 정렬 (SORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sort 와 orderBy 함수는 동일한 효과를 가집니다\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# sort 와 orderBy 함수는 동일한 효과를 가집니다\")\n",
    "df.sort(\"count\").show(2)\n",
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(2)\n",
    "df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# asc_nulls_first, desc_nulls_first, asc_nulls_last, desc_nulls_last 메서드로 null의 정렬 순서를 지정\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Algeria|      United States|    4|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Algeria|      United States|    4|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Algeria|      United States|    4|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "print(\"# asc_nulls_first, desc_nulls_first, asc_nulls_last, desc_nulls_last 메서드로 null의 정렬 순서를 지정\")\n",
    "df.sort(\"DEST_COUNTRY_NAME\").show(1)\n",
    "df.sort(df[\"DEST_COUNTRY_NAME\"].asc_nulls_first()).show(1)\n",
    "df.sort(df.DEST_COUNTRY_NAME.asc_nulls_first()).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 정렬의 경우 예약어 컬럼명에 유의해야 하므로, expr 을 사용하거나, 명시적으로 구조화 API 를 사용하는 것도 좋습니다\n",
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Vietnam|    2|\n",
      "|    United States|          Venezuela|  246|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|             Angola|   13|\n",
      "|    United States|           Anguilla|   38|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# 정렬의 경우 예약어 컬럼명에 유의해야 하므로, expr 을 사용하거나, 명시적으로 구조화 API 를 사용하는 것도 좋습니다\") \n",
    "from pyspark.sql.functions import desc, asc\n",
    "df.orderBy(df[\"count\"].desc()).show(2)\n",
    "df.orderBy(df.ORIGIN_COUNTRY_NAME.desc(), df.DEST_COUNTRY_NAME.asc()).show(2)\n",
    "df.orderBy(expr(\"ORIGIN_COUNTRY_NAME DESC\"), expr(\"DEST_COUNTRY_NAME ASC\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11 로우 수 제한 (LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(5).show()\n",
    "df.orderBy(expr(\"count desc\")).limit(6).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>5. [고급]</font> \"data/flight-data/json/2015-summary.json\" 경로의 JSON 데이터를 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 10건의 데이터를 출력하세요\n",
    "#### 3. count 를 100으로 나눈 몫을 가지는 cnt 컬럼을 추가합니다 표현식은 다음과 같습니다 `expr(\"floor(count / 100)\")` - 힌트: withColumn(\"컬럼명\", \"표현식\")\n",
    "#### 4. cnt 컬럼을 기준으로 내림차순 정렬하되, 동순위가 발생하는 경우 ORIGIN_COUNTRY_NAME 오름차순, DEST_COUNTRY_NAME 내림차순으로 정렬하여 출력하세요\n",
    "#### 5. 출력된 결과의 상위 10개만 제한하여 (limit) display 함수로 출력하세요\n",
    "\n",
    "<details><summary>[실습5] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "df5 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(\"data/flight-data/json/2015-summary.json\")\n",
    ")\n",
    "    \n",
    "df5.printSchema()\n",
    "answer = df5.withColumn(\"cnt\", expr(\"floor(count / 100)\")).orderBy(desc(\"cnt\"), asc(\"ORIGIN_COUNTRY_NAME\"), desc(\"DEST_COUNTRY_NAME\")).limit(10)\n",
    "display(answer)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>DEST_COUNTRY_NAME</th><th>ORIGIN_COUNTRY_NAME</th><th>count</th><th>cnt</th></tr>\n",
       "<tr><td>United States</td><td>United States</td><td>370002</td><td>3700</td></tr>\n",
       "<tr><td>United States</td><td>Canada</td><td>8483</td><td>84</td></tr>\n",
       "<tr><td>Canada</td><td>United States</td><td>8399</td><td>83</td></tr>\n",
       "<tr><td>United States</td><td>Mexico</td><td>7187</td><td>71</td></tr>\n",
       "<tr><td>Mexico</td><td>United States</td><td>7140</td><td>71</td></tr>\n",
       "<tr><td>United Kingdom</td><td>United States</td><td>2025</td><td>20</td></tr>\n",
       "<tr><td>United States</td><td>United Kingdom</td><td>1970</td><td>19</td></tr>\n",
       "<tr><td>Japan</td><td>United States</td><td>1548</td><td>15</td></tr>\n",
       "<tr><td>United States</td><td>Dominican Republic</td><td>1420</td><td>14</td></tr>\n",
       "<tr><td>United States</td><td>Japan</td><td>1496</td><td>14</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+-------------------+------+----+\n",
       "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count| cnt|\n",
       "+-----------------+-------------------+------+----+\n",
       "|    United States|      United States|370002|3700|\n",
       "|    United States|             Canada|  8483|  84|\n",
       "|           Canada|      United States|  8399|  83|\n",
       "|    United States|             Mexico|  7187|  71|\n",
       "|           Mexico|      United States|  7140|  71|\n",
       "|   United Kingdom|      United States|  2025|  20|\n",
       "|    United States|     United Kingdom|  1970|  19|\n",
       "|            Japan|      United States|  1548|  15|\n",
       "|    United States| Dominican Republic|  1420|  14|\n",
       "|    United States|              Japan|  1496|  14|\n",
       "+-----------------+-------------------+------+----+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df5 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(\"data/flight-data/json/2015-summary.json\")\n",
    ")\n",
    "\n",
    "df5.printSchema()\n",
    "answer = df5.withColumn(\"cnt\", expr(\"floor(count / 100)\")).orderBy(desc(\"cnt\"), asc(\"ORIGIN_COUNTRY_NAME\"), desc(\"DEST_COUNTRY_NAME\")).limit(10)\n",
    "display(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 기타 데이터 프레임 연산자\n",
    "\n",
    "### 5.1 사전에 스키마를 정의하는 장점\n",
    "* 데이터 타입을 추론에 대한 신경을 쓸 필요가 없다\n",
    "* 스키마 추론을 위한 별도의 작업에 드는 리소스를 줄일 수 있다\n",
    "* 스키마에 맞지 않는 데이터의 오류를 빠르게 인지할 수 있다\n",
    "\n",
    "### 5.2 스키마를 정의하는 두 가지 방법\n",
    "* 1. 프로그래밍 방식으로 정의하는 방법\n",
    "* 2. DDL 구문을 이용하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1. Programming Style\n",
      "StructType(List(StructField(author,StringType,false),StructField(title,StringType,false),StructField(pages,IntegerType,false)))\n",
      "root\n",
      " |-- author: string (nullable = false)\n",
      " |-- title: string (nullable = false)\n",
      " |-- pages: integer (nullable = false)\n",
      "\n",
      "+----------+----------------------------+-----+\n",
      "|author    |title                       |pages|\n",
      "+----------+----------------------------+-----+\n",
      "|정휘센    |안녕하세요 정휘센 입니다    |300  |\n",
      "|김싸이언  |안녕하세요 김싸이언 입니다  |200  |\n",
      "|유코드제로|안녕하세요 유코드제로 입니다|100  |\n",
      "+----------+----------------------------+-----+\n",
      "\n",
      "\n",
      "# 2. DDL Style\n",
      "`author` string, `title` string, `pages` int\n",
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- pages: integer (nullable = true)\n",
      "\n",
      "+----------+----------------------------+-----+\n",
      "|author    |title                       |pages|\n",
      "+----------+----------------------------+-----+\n",
      "|정휘센    |안녕하세요 정휘센 입니다    |300  |\n",
      "|김싸이언  |안녕하세요 김싸이언 입니다  |200  |\n",
      "|유코드제로|안녕하세요 유코드제로 입니다|100  |\n",
      "+----------+----------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "\n",
    "data = [\n",
    "    [\"정휘센\", \"안녕하세요 정휘센 입니다\", 300],\n",
    "    [\"김싸이언\", \"안녕하세요 김싸이언 입니다\", 200],\n",
    "    [\"유코드제로\", \"안녕하세요 유코드제로 입니다\", 100]\n",
    "]\n",
    "\n",
    "print(\"# 1. Programming Style\")\n",
    "schema1 = StructType([\n",
    "    StructField(\"author\", StringType(), False),\n",
    "    StructField(\"title\", StringType(), False),\n",
    "    StructField(\"pages\", IntegerType(), False),\n",
    "])\n",
    "print(schema1)\n",
    "df1 = spark.createDataFrame(data, schema1)\n",
    "df1.printSchema()\n",
    "df1.show(truncate=False)\n",
    "\n",
    "rows = [\n",
    "    Row(\"정휘센\", \"안녕하세요 정휘센 입니다\", 300),\n",
    "    Row(\"김싸이언\", \"안녕하세요 김싸이언 입니다\", 200),\n",
    "    Row(\"유코드제로\", \"안녕하세요 유코드제로 입니다\", 100)\n",
    "]\n",
    "\n",
    "print(\"\\n# 2. DDL Style\")\n",
    "schema2 = \"`author` string, `title` string, `pages` int\"\n",
    "print(schema2)\n",
    "df2 = spark.createDataFrame(rows, schema2)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "assert(df1.subtract(df2).count() == 0)\n",
    "assert(df2.subtract(df1).count() == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>6. [고급]</font> Row 와 문자열을 통한 스키마 구현을 통해 데이터 프레임을 생성하세요\n",
    "#### 1. 스키마 : id int, name string, payment int\n",
    "#### 2. 임의의 데이터를 3건 정도 생성해서 데이터프레임을 만들어 보세요\n",
    "#### 3. 스키마를 출력하세요\n",
    "#### 4. 데이터를 출력하세요\n",
    "\n",
    "<details><summary>[실습6] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "df6 = [\n",
    "    Row(1, \"엘지전자\", 1000),\n",
    "    Row(2, \"엘지화학\", 2000),\n",
    "    Row(3, \"엘지디스플레이\", 3000)\n",
    "]\n",
    "sc6 = \"`id` int, `name` string, `payment` int\"\n",
    "answer = spark.createDataFrame(df6, sc6)\n",
    "answer.printSchema()\n",
    "display(answer)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- payment: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>name</th><th>payment</th></tr>\n",
       "<tr><td>1</td><td>엘지전자</td><td>1000</td></tr>\n",
       "<tr><td>2</td><td>엘지화학</td><td>2000</td></tr>\n",
       "<tr><td>3</td><td>엘지디스플레이</td><td>3000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+--------------+-------+\n",
       "| id|          name|payment|\n",
       "+---+--------------+-------+\n",
       "|  1|      엘지전자|   1000|\n",
       "|  2|      엘지화학|   2000|\n",
       "|  3|엘지디스플레이|   3000|\n",
       "+---+--------------+-------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df6 = [\n",
    "    Row(1, \"엘지전자\", 1000),\n",
    "    Row(2, \"엘지화학\", 2000),\n",
    "    Row(3, \"엘지디스플레이\", 3000)\n",
    "]\n",
    "sc6 = \"`id` int, `name` string, `payment` int\"\n",
    "answer = spark.createDataFrame(df6, sc6)\n",
    "answer.printSchema()\n",
    "display(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 중첩된 배열 스키마"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+---+-----+-----+-----------------+---------+----+-------------------+\n",
      "|Id |First|Last |Url              |Published|Hits|Campaigns          |\n",
      "+---+-----+-----+-----------------+---------+----+-------------------+\n",
      "|1  |Jules|Damji|https://tinyurl.1|1/4/2016 |4535|[twitter, LinkedIn]|\n",
      "+---+-----+-----+-----------------+---------+----+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Id\", IntegerType(), False),\n",
    "    StructField(\"First\", StringType(), False),\n",
    "    StructField(\"Last\", StringType(), False),\n",
    "    StructField(\"Url\", StringType(), False),\n",
    "    StructField(\"Published\", StringType(), False),\n",
    "    StructField(\"Hits\", IntegerType(), False),\n",
    "    StructField(\"Campaigns\", ArrayType(StringType()), False),\n",
    "])\n",
    "blogDF = spark.read.schema(schema).json(\"data/learning-spark/blogs.json\")\n",
    "blogDF.printSchema()\n",
    "blogDF.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 컬럼과 표현식\n",
    "> 컬럼은 공용 메소드들을 가진 객체들이며, pyspark.sql.functions.expr() 함수를 이용하여 표현식을 그대로 사용할 수 있습니다\n",
    "\n",
    "* 특히 컬럼 함수를 통해 다양한 연산자를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Id', 'First', 'Last', 'Url', 'Published', 'Hits', 'Campaigns']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import Column\n",
    "print(blogDF.columns)\n",
    "# help(Column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|      AuthorsId|\n",
      "+---------------+\n",
      "|  Jules.Damji@1|\n",
      "| Brooke.Wenig@2|\n",
      "|    Denny.Lee@3|\n",
      "|Tathagata.Das@4|\n",
      "+---------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogDF.withColumn(\"AuthorsId\", (concat(expr(\"First\"), lit(\".\"), expr(\"Last\"), lit(\"@\"), expr(\"Id\"))))\\\n",
    ".select(col(\"AuthorsId\"))\\\n",
    ".show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogDF.select(expr(\"Hits\")).show(2)\n",
    "blogDF.select(col(\"Hits\")).show(2)\n",
    "blogDF.select(\"Hits\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogDF.sort(col(\"Id\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 로우 생성 및 다루기\n",
    "* 로우의 경우 컬럼을 인덱스를 기준으로 접근할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reynold\n",
      "+-------------+-----+\n",
      "|      Authors|State|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "blog_row = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\",\n",
    "[\"twitter\", \"LinkedIn\"])\n",
    "print(blog_row[1])\n",
    "\n",
    "rows = [Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\", \"CA\")]\n",
    "authors_df = spark.createDataFrame(rows, [\"Authors\", \"State\"])\n",
    "authors_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+----------------+----------+-------------+\n",
      "|CallNumber|UnitID|IncidentNumber|CallType        |CallDate  |RowID        |\n",
      "+----------+------+--------------+----------------+----------+-------------+\n",
      "|20110016  |T13   |2003235       |Structure Fire  |01/11/2002|020110016-T13|\n",
      "|20110022  |M17   |2003241       |Medical Incident|01/11/2002|020110022-M17|\n",
      "|20110023  |M41   |2003242       |Medical Incident|01/11/2002|020110023-M41|\n",
      "|20110032  |E11   |2003250       |Vehicle Fire    |01/11/2002|020110032-E11|\n",
      "|20110043  |B04   |2003259       |Alarms          |01/11/2002|020110043-B04|\n",
      "|20110072  |T08   |2003279       |Structure Fire  |01/11/2002|020110072-T08|\n",
      "|20110125  |E33   |2003301       |Alarms          |01/11/2002|020110125-E33|\n",
      "|20110130  |E36   |2003304       |Alarms          |01/11/2002|020110130-E36|\n",
      "|20110197  |E05   |2003343       |Medical Incident|01/11/2002|020110197-E05|\n",
      "|20110215  |E06   |2003348       |Medical Incident|01/11/2002|020110215-E06|\n",
      "+----------+------+--------------+----------------+----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python, define a schema\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Programmatic way to define a schema\n",
    "fire_schema = StructType([StructField('CallNumber', IntegerType(), True),\n",
    "StructField('UnitID', StringType(), True),\n",
    "StructField('IncidentNumber', IntegerType(), True),\n",
    "StructField('CallType', StringType(), True),\n",
    "StructField('CallDate', StringType(), True),\n",
    "StructField('WatchDate', StringType(), True),\n",
    "StructField('CallFinalDisposition', StringType(), True),\n",
    "StructField('AvailableDtTm', StringType(), True),\n",
    "StructField('Address', StringType(), True),\n",
    "StructField('City', StringType(), True),\n",
    "StructField('Zipcode', IntegerType(), True),\n",
    "StructField('Battalion', StringType(), True),\n",
    "StructField('StationArea', StringType(), True),\n",
    "StructField('Box', StringType(), True),\n",
    "StructField('OriginalPriority', StringType(), True),\n",
    "StructField('Priority', StringType(), True),\n",
    "StructField('FinalPriority', IntegerType(), True),\n",
    "StructField('ALSUnit', BooleanType(), True),\n",
    "StructField('CallTypeGroup', StringType(), True),\n",
    "StructField('NumAlarms', IntegerType(), True),\n",
    "StructField('UnitType', StringType(), True),\n",
    "StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n",
    "StructField('FirePreventionDistrict', StringType(), True),\n",
    "StructField('SupervisorDistrict', StringType(), True),\n",
    "StructField('Neighborhood', StringType(), True),\n",
    "StructField('Location', StringType(), True),\n",
    "StructField('RowID', StringType(), True),\n",
    "StructField('Delay', FloatType(), True)])\n",
    "\n",
    "# Use the DataFrameReader interface to read a CSV file\n",
    "sf_fire_file = \"data/learning-spark/sf-fire-calls.csv\"\n",
    "fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)\n",
    "fire_df.select(\"CallNumber\", \"UnitID\", \"IncidentNumber\", \"CallType\", \"CallDate\", \"RowID\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 파케이 파일 혹은 테이블 저장\n",
    "* save 저장 시에는 해당 경로에 파케이 파일이 저장되고, saveAsTable 저장 시에는 \"spark.sql.warehouse.dir\" 의 위치에 생성됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetPath=\"target/sf_fire_calls\"\n",
    "fire_df.write.format(\"parquet\").mode(\"overwrite\").save(parquetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf \"/home/jovyan/work/spark-warehouse/sf_fire_calls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetTable=\"sf_fire_calls\"\n",
    "# spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n",
    "fire_df.write.format(\"parquet\").saveAsTable(parquetTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 프로젝션과 필터\n",
    "> *Projection*은 특정 관계형 조건 혹은 필터에 매칭되는 로우에 대해서만 반환하는 것을 말합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+-----------------------------+\n",
      "|IncidentNumber|         AvailableDtTm|                     CallType|\n",
      "+--------------+----------------------+-----------------------------+\n",
      "|       2003235|01/11/2002 01:51:44 AM|               Structure Fire|\n",
      "|       2003250|01/11/2002 04:16:46 AM|                 Vehicle Fire|\n",
      "|       2003259|01/11/2002 06:01:58 AM|                       Alarms|\n",
      "|       2003279|01/11/2002 08:03:26 AM|               Structure Fire|\n",
      "|       2003301|01/11/2002 09:46:44 AM|                       Alarms|\n",
      "|       2003304|01/11/2002 09:58:53 AM|                       Alarms|\n",
      "|       2003382|01/11/2002 02:59:04 PM|               Structure Fire|\n",
      "|       2003408|01/11/2002 04:09:08 PM|               Structure Fire|\n",
      "|       2003408|01/11/2002 04:09:08 PM|               Structure Fire|\n",
      "|       2003408|01/11/2002 04:09:08 PM|               Structure Fire|\n",
      "|       2003429|01/11/2002 05:17:15 PM|     Odor (Strange / Unknown)|\n",
      "|       2003453|01/11/2002 06:48:01 PM|                       Alarms|\n",
      "|       2003497|01/11/2002 09:03:17 PM|               Structure Fire|\n",
      "|       2003554|01/12/2002 01:56:32 AM|               Structure Fire|\n",
      "|       2003618|01/12/2002 11:07:36 AM|     Odor (Strange / Unknown)|\n",
      "|       2003649|01/12/2002 01:03:10 PM|     Odor (Strange / Unknown)|\n",
      "|       2003695|01/12/2002 04:46:59 PM|               Structure Fire|\n",
      "|       2003756|01/12/2002 07:54:42 PM|                       Alarms|\n",
      "|       2003770|01/12/2002 08:44:01 PM|Smoke Investigation (Outside)|\n",
      "|       2003777|01/12/2002 09:14:13 PM|               Structure Fire|\n",
      "+--------------+----------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------+----------------------+--------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType      |\n",
      "+--------------+----------------------+--------------+\n",
      "|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n",
      "|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n",
      "|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n",
      "|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n",
      "|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------------+\n",
      "|ResponseDelayedinMins|\n",
      "+---------------------+\n",
      "|5.35                 |\n",
      "|6.25                 |\n",
      "|5.2                  |\n",
      "|5.6                  |\n",
      "|7.25                 |\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_fire_df = (\n",
    "    fire_df\n",
    "    .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\")\n",
    "    .where(col(\"CallType\") != \"Medical Incident\")\n",
    ")\n",
    "print(few_fire_df)\n",
    "few_fire_df.show(5, truncate=False)\n",
    "\n",
    "new_fire_df = fire_df.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "(\n",
    "    new_fire_df\n",
    "    .select(\"ResponseDelayedinMins\")\n",
    "    .where(col(\"ResponseDelayedinMins\") > 5)\n",
    "    .show(5, False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8 날짜 관련 함수\n",
    "* 날짜의 경우 문자열로 전달되고 있기 때문에 표현 및 활용을 위해서는 to_timestamp(), to_date() 와 같은 날짜관련 함수를 사용할 수 있습니다.\n",
    "  - 한번 timestamp 형태로 변경된 컬럼에 대해서는 year, month, dayofmonth 와 같은 일자관련 함수를 통해 다양한 예제를 실습할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|IncidentDate       |OnWatchDate        |AvailableDtTS      |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 06:01:58|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------------+-------------------+------------------------+\n",
      "|year(IncidentDate)|month(IncidentDate)|dayofmonth(IncidentDate)|\n",
      "+------------------+-------------------+------------------------+\n",
      "|              2000|                  4|                      12|\n",
      "|              2000|                  4|                      13|\n",
      "|              2000|                  4|                      14|\n",
      "|              2000|                  4|                      15|\n",
      "|              2000|                  4|                      16|\n",
      "+------------------+-------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_ts_df = (new_fire_df\n",
    "    .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    "    .drop(\"CallDate\")\n",
    "    .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    "    .drop(\"WatchDate\")\n",
    "    .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"),\"MM/dd/yyyy hh:mm:ss a\"))\n",
    "    .drop(\"AvailableDtTm\")\n",
    ")\n",
    "fire_ts_df.select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\").show(5, truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "(\n",
    "    fire_ts_df\n",
    "    .select(year('IncidentDate'), month('IncidentDate'), dayofmonth(\"IncidentDate\"))\n",
    "    .distinct()\n",
    "    .orderBy(year('IncidentDate'), month('IncidentDate'), dayofmonth(\"IncidentDate\"))\n",
    "    .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>7. [기본]</font> 아래와 같이 제공된 데이터를 통해 생성한 데이터를 파케이 포맷으로 저장하세요\n",
    "#### 1. \"target/lgde_user\" 경로에 파케이 파일로 저장하세요\n",
    "#### 2. \"lgde_user\" 테이블로 저장하세요\n",
    "\n",
    "<details><summary>[실습7] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df6 = [\n",
    "    Row(1, \"엘지전자\", 1000),\n",
    "    Row(2, \"엘지화학\", 2000),\n",
    "    Row(3, \"엘지디스플레이\", 3000)\n",
    "]\n",
    "sc6 = \"`id` int, `name` string, `payment` int\"\n",
    "answer = spark.createDataFrame(df6, sc6)\n",
    "answer.printSchema()\n",
    "display(answer)\n",
    "\n",
    "answer.write.format(\"parquet\").save(\"target/lgde_user\")\n",
    "answer.write.format(\"parquet\").saveAsTable(\"lgde_user\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- payment: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>name</th><th>payment</th></tr>\n",
       "<tr><td>1</td><td>엘지전자</td><td>1000</td></tr>\n",
       "<tr><td>2</td><td>엘지화학</td><td>2000</td></tr>\n",
       "<tr><td>3</td><td>엘지디스플레이</td><td>3000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+--------------+-------+\n",
       "| id|          name|payment|\n",
       "+---+--------------+-------+\n",
       "|  1|      엘지전자|   1000|\n",
       "|  2|      엘지화학|   2000|\n",
       "|  3|엘지디스플레이|   3000|\n",
       "+---+--------------+-------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df6 = [\n",
    "    Row(1, \"엘지전자\", 1000),\n",
    "    Row(2, \"엘지화학\", 2000),\n",
    "    Row(3, \"엘지디스플레이\", 3000)\n",
    "]\n",
    "sc6 = \"`id` int, `name` string, `payment` int\"\n",
    "answer = spark.createDataFrame(df6, sc6)\n",
    "answer.printSchema()\n",
    "display(answer)\n",
    "\n",
    "answer.write.format(\"parquet\").save(\"target/lgde_user\")\n",
    "answer.write.format(\"parquet\").saveAsTable(\"lgde_user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>8. [기본]</font> '실습7' 에서 생성한 데이터를 읽어서 출력하세요\n",
    "#### 1. \"target/lgde_user\" 경로에 파케이 파일을 읽어서 스키마와 데이터를 출력하세요\n",
    "#### 2. \"lgde_user\" 테이블로 저장된 테이블을 spark sql 로 읽어서 스키마와 데이터를 출력하세요\n",
    "\n",
    "<details><summary>[실습8] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df8 = (\n",
    "    spark\n",
    "    .read.parquet(\"target/lgde_user\")\n",
    ")\n",
    "df8.printSchema()\n",
    "display(df8)\n",
    "\n",
    "answer = spark.sql(\"select * from lgde_user\")\n",
    "display(answer)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- payment: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>name</th><th>payment</th></tr>\n",
       "<tr><td>3</td><td>엘지디스플레이</td><td>3000</td></tr>\n",
       "<tr><td>1</td><td>엘지전자</td><td>1000</td></tr>\n",
       "<tr><td>2</td><td>엘지화학</td><td>2000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+--------------+-------+\n",
       "| id|          name|payment|\n",
       "+---+--------------+-------+\n",
       "|  3|엘지디스플레이|   3000|\n",
       "|  1|      엘지전자|   1000|\n",
       "|  2|      엘지화학|   2000|\n",
       "+---+--------------+-------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>name</th><th>payment</th></tr>\n",
       "<tr><td>3</td><td>엘지디스플레이</td><td>3000</td></tr>\n",
       "<tr><td>1</td><td>엘지전자</td><td>1000</td></tr>\n",
       "<tr><td>2</td><td>엘지화학</td><td>2000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+--------------+-------+\n",
       "| id|          name|payment|\n",
       "+---+--------------+-------+\n",
       "|  3|엘지디스플레이|   3000|\n",
       "|  1|      엘지전자|   1000|\n",
       "|  2|      엘지화학|   2000|\n",
       "+---+--------------+-------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df8 = (\n",
    "    spark\n",
    "    .read.parquet(\"target/lgde_user\")\n",
    ")\n",
    "df8.printSchema()\n",
    "display(df8)\n",
    "\n",
    "answer = spark.sql(\"select * from lgde_user\")\n",
    "display(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 데이터셋 API\n",
    "> Python 과 R 은 compile-time type-safe 하지 않기 때문에, Datasets 통한 Typed 데이터 타입을 사용할 수 없습니다. Datasets 을 이용하는 경우에도 Spark SQL 엔진이 객체를 생성, 변환, 직렬화, 역직렬화를 수행하며, **Dataframe 의 경우와 마찬가지로 Off-heap 을 통한 메모리 관리를 수행**하게 되며, Dataset encoders 를 이용합니다\n",
    "\n",
    "### 6.1 데이터셋과 데이터프레임 비교\n",
    "\n",
    "| Structured APIs | SQL vs. Dataframe vs. Datasets |\n",
    "|---|---|\n",
    "| ![structured-api](images/structured-api.png) | ![sql-vs-dataframes-vs-datasets-type-safety-spectrum](images/sql-vs-dataframes-vs-datasets-type-safety-spectrum.png) |\n",
    "\n",
    "* 언어별 타입 객체 비교\n",
    "![typed-untyped](images/typed-untyped.png)\n",
    "\n",
    "* Scala: Case Class 를 통해 선언\n",
    "```scala\n",
    "case class DeviceIoTData (\n",
    "    battery_level: Long, \n",
    "    c02_level: Long,\n",
    "    cca2: String, \n",
    "    cca3: String, \n",
    "    cn: String, \n",
    "    device_id: Long,\n",
    "    device_name: String, \n",
    "    humidity: Long, \n",
    "    ip: String, \n",
    "    latitude: Double,\n",
    "    lcd: String, \n",
    "    longitude: Double, \n",
    "    scale:String, \n",
    "    temp: Long,\n",
    "    timestamp: Long)\n",
    "```\n",
    "\n",
    "* 데이터를 읽고 DeviceIoTData 클래스로 변환을 수행합니다\n",
    "```scala\n",
    "val ds = spark.read.json(\"/databricks-datasets/learning-spark-v2/iot-devices/iot_devices.json\").as[DeviceIoTData]\n",
    "val filterTempDS = ds.filter({d => {d.temp > 30 && d.humidity > 70})\n",
    "```\n",
    "* Datasets 이용 시에는 filter(), map(), groupBy(), select(), take() 등의 일반적인 함수를 사용합니다\n",
    "```scala                              \n",
    "case class DeviceTempByCountry(temp: Long, device_name: String, device_id: Long, cca3: String)\n",
    "val dsTemp = ds.filter(d => {d.temp > 25})\n",
    "    .map(d => (d.temp, d.device_name, d.device_id, d.cca3))\n",
    "    .toDF(\"temp\", \"device_name\", \"device_id\", \"cca3\")\n",
    "    .as[DeviceTempByCountry]\n",
    "```\n",
    "\n",
    "### 6.2 데이터셋 데이터프레임 그리고 RDD\n",
    "* Datasets\n",
    "  - compile-time 의 type safety 가 필요한 경우\n",
    "* Dataframe\n",
    "  - SQL-like 쿼리를 이용하고자 하는 경우\n",
    "  - 통합, 코드 최적화 그리고 API를 활용한 모듈화를 원하는 경우\n",
    "  - R 혹은 Python 을 이용해야 하는 경우\n",
    "  - 공간, 속도 효율성을 고려해야 하는 경우\n",
    "* RDD\n",
    "  - 별도의 RDD를 이용하는 써드파티 패키지를 사용하는 경우\n",
    "  - 코드, 공간, 속도 최적화 등을 원하지 않는 경우\n",
    "  - 스파크가 수행할 쿼리를 정확히 지시해야만 할 때\n",
    "\n",
    "\n",
    "* RDD와 데이터프레임과 데이터셋은 서로 다른가?\n",
    "  - 데이터프레임과 데이터셋은 RDD 위에서 구현됩니다. 즉, whole-stage code generation 단계에서 압축된 RDD 코드로 분해됩니다.\n",
    "\n",
    "> DataFrames and Datasets are\n",
    "built on top of RDDs, and they get decomposed to compact RDD code during wholestage\n",
    "code generation, which we discuss in the next section\n",
    "\n",
    "* Spark SQL\n",
    "![spark-sql](images/spark-sql.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 카탈리스트 옵티마이저 (참고용)\n",
    "> Spark SQL 엔진의 핵심이며 크게 4가지 단계로 구분됩니다. \n",
    "\n",
    "### 7.1 분석 (Analysis)\n",
    "* \"추상화 구문 트리(AST, Abstract Syntax Tree)\" 생성 단계로, 모든 테이블명과 컬럼명은 내부적으로 컬럼명, 데이터유형, 함수와 더불어 데이터베이스와 테이블 이름까지 모두 관리하고 있는 *Catalog*에 의해 해석되어 트리 형태의 구조로 생성됩니다\n",
    "\n",
    "### 7.2 논리 최적화 (Logical Optimization)\n",
    "* 카탈리스트 옵티마이저는 우선 다수의 논리적 계획을 세우고, \"비용 기반 옵티마이저(CBO, Cost-Based Optimizer)\"를 이용하여 각 계획에 비용(Cost)를 할당합니다. 이러한 계획은 아래의 \"Figure 3-5\"와 같은 연산자 트리 형태로 구성되며, 이때에 **constant folding, predicate pushdown, projection pruning, Boolean expression simplification** 등의 최적화가 이루어집니다\n",
    "\n",
    "### 7.3 물리 계획 (Physical Planning)\n",
    "* Spark SQL 엔진은 CBO에 의해 선택된 논리 계획에 대해 스파크 엔진에 존재하는 적절한 연산자들을 이용하여 최적의 계획을 생성합니다\n",
    "\n",
    "### 7.4 코드 생성 (Code Generation)\n",
    "* 마지막 단계에서는  Project Tungsten 의 whole-stage code generation 을 통해 [마치 컴파일러와 같이 동작](https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html)하며, 메모리 상에 로딩된 데이터 집합에 대해 수행될 최적의 자바 바이트 코드를 생성해 냅니다.\n",
    "\n",
    "* What is ***whole-stage code generation***?\n",
    "  - 물리적인 쿼리 최적화 단계를 말하며, 쿼리 전체를 하나의 함수로 만들어 냅니다\n",
    "  - virtual function call 을 제거하거나, 중간 데이터를 CPU registers 에 올리는 등의 최적화 작업을 수행합니다\n",
    "  - Spark 2.0 텅스텐 엔진은 압축된 RDD 코드를 생성하는 방식으로 개선 되었습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['Total DESC NULLS LAST], true\n",
      "+- Aggregate [State#1600, Color#1601, Count#1602], [State#1600, Color#1601, Count#1602, sum(cast(Count#1602 as bigint)) AS Total#1613L]\n",
      "   +- Project [State#1600, Color#1601, Count#1602]\n",
      "      +- Relation[State#1600,Color#1601,Count#1602] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "State: string, Color: string, Count: int, Total: bigint\n",
      "Sort [Total#1613L DESC NULLS LAST], true\n",
      "+- Aggregate [State#1600, Color#1601, Count#1602], [State#1600, Color#1601, Count#1602, sum(cast(Count#1602 as bigint)) AS Total#1613L]\n",
      "   +- Project [State#1600, Color#1601, Count#1602]\n",
      "      +- Relation[State#1600,Color#1601,Count#1602] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [Total#1613L DESC NULLS LAST], true\n",
      "+- Aggregate [State#1600, Color#1601, Count#1602], [State#1600, Color#1601, Count#1602, sum(cast(Count#1602 as bigint)) AS Total#1613L]\n",
      "   +- Relation[State#1600,Color#1601,Count#1602] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Sort [Total#1613L DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(Total#1613L DESC NULLS LAST, 200), true, [id=#1137]\n",
      "   +- *(2) HashAggregate(keys=[State#1600, Color#1601, Count#1602], functions=[sum(cast(Count#1602 as bigint))], output=[State#1600, Color#1601, Count#1602, Total#1613L])\n",
      "      +- Exchange hashpartitioning(State#1600, Color#1601, Count#1602, 200), true, [id=#1133]\n",
      "         +- *(1) HashAggregate(keys=[State#1600, Color#1601, Count#1602], functions=[partial_sum(cast(Count#1602 as bigint))], output=[State#1600, Color#1601, Count#1602, sum#1619L])\n",
      "            +- FileScan csv [State#1600,Color#1601,Count#1602] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/work/data/databricks/mnm_dataset.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<State:string,Color:string,Count:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnm_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"data/databricks/mnm_dataset.csv\")\n",
    ")\n",
    "count_mnm_df = (\n",
    "    mnm_df.select(\"State\", \"Color\", \"Count\")\n",
    "    .groupBy(\"State\", \"Color\", \"Count\")\n",
    "    .agg(sum(\"Count\")\n",
    "    .alias(\"Total\"))\n",
    "    .orderBy(\"Total\", ascending=False)\n",
    ")\n",
    "count_mnm_df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['Total DESC NULLS LAST], true\n",
      "+- 'Aggregate ['State, 'Color, 'Count], ['State, 'Color, 'Count, 'sum('Count) AS Total#1620]\n",
      "   +- 'UnresolvedRelation [mnm_dataset]\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "State: string, Color: string, Count: int, Total: bigint\n",
      "Sort [Total#1620L DESC NULLS LAST], true\n",
      "+- Aggregate [State#1600, Color#1601, Count#1602], [State#1600, Color#1601, Count#1602, sum(cast(Count#1602 as bigint)) AS Total#1620L]\n",
      "   +- SubqueryAlias mnm_dataset\n",
      "      +- Relation[State#1600,Color#1601,Count#1602] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [Total#1620L DESC NULLS LAST], true\n",
      "+- Aggregate [State#1600, Color#1601, Count#1602], [State#1600, Color#1601, Count#1602, sum(cast(Count#1602 as bigint)) AS Total#1620L]\n",
      "   +- Relation[State#1600,Color#1601,Count#1602] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Sort [Total#1620L DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(Total#1620L DESC NULLS LAST, 200), true, [id=#1170]\n",
      "   +- *(2) HashAggregate(keys=[State#1600, Color#1601, Count#1602], functions=[sum(cast(Count#1602 as bigint))], output=[State#1600, Color#1601, Count#1602, Total#1620L])\n",
      "      +- Exchange hashpartitioning(State#1600, Color#1601, Count#1602, 200), true, [id=#1166]\n",
      "         +- *(1) HashAggregate(keys=[State#1600, Color#1601, Count#1602], functions=[partial_sum(cast(Count#1602 as bigint))], output=[State#1600, Color#1601, Count#1602, sum#1628L])\n",
      "            +- FileScan csv [State#1600,Color#1601,Count#1602] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/work/data/databricks/mnm_dataset.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<State:string,Color:string,Count:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnm_df.createOrReplaceTempView(\"mnm_dataset\")\n",
    "count_mnm_df = spark.sql(\"\"\"\n",
    "    SELECT State, Color, Count, sum(Count) AS Total\n",
    "    FROM mnm_dataset\n",
    "    GROUP BY State, Color, Count\n",
    "    ORDER BY Total DESC\n",
    "\"\"\")\n",
    "count_mnm_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 아래와 같이 2개의 테이블에 대해 조인, 필터, 프로젝션 등의 연산 시에 아래와 같은 최적화로 **Disk 및 Network I/O 를 줄일 수 있습니다**.\n",
    "  - Predicate Pushdown : 데이터 소스를 모두 읽지 않고, 필터 조건에 해당하는 데이터만 읽습니다\n",
    "  - Column Pruning : 데이터 소스에서 모든 필드를 읽지 않고, 필요한 필터만 읽습니다\n",
    "\n",
    "```scala\n",
    "val users = spark.read.parquet(\"/users/parquet/path\")\n",
    "val events = spark.read.parquet(\"/events/parquet/path\")\n",
    "val joinedDF = users.join(events, users(\"id\") === events(\"uid\"))\n",
    ".filter(events(\"date\") > \"2015-01-01\")\n",
    "```\n",
    "\n",
    "![query-transformation](images/query-transformation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>9. [중급]</font> \"data/tbl_user.csv\" 에 저장된 CSV 파일을 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 데이터를 10건 출력하세요\n",
    "#### 3. 가입일자 컬럼(u_signup)을 이용하여 가장 최근에 가입한 5명을 출력하세요\n",
    "\n",
    "<details><summary>[실습9] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df9 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"data/tbl_user.csv\")\n",
    ")\n",
    "df9.printSchema()\n",
    "df9.show(10)\n",
    "df9.createOrReplaceTempView(\"tbl_user\")\n",
    "answer = spark.sql(\"select * from tbl_user order by u_signup desc limit 5\")\n",
    "display(answer)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- u_id: integer (nullable = true)\n",
      " |-- u_name: string (nullable = true)\n",
      " |-- u_gender: string (nullable = true)\n",
      " |-- u_signup: integer (nullable = true)\n",
      "\n",
      "+----+----------+--------+--------+\n",
      "|u_id|    u_name|u_gender|u_signup|\n",
      "+----+----------+--------+--------+\n",
      "|   1|    정휘센|      남|19700808|\n",
      "|   2|  김싸이언|      남|19710201|\n",
      "|   3|    박트롬|      여|19951030|\n",
      "|   4|    청소기|      남|19770329|\n",
      "|   5|유코드제로|      여|20021029|\n",
      "|   6|  윤디오스|      남|20040101|\n",
      "|   7|  임모바일|      남|20040807|\n",
      "|   8|  조노트북|      여|20161201|\n",
      "|   9|  최컴퓨터|      남|20201124|\n",
      "+----+----------+--------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>u_id</th><th>u_name</th><th>u_gender</th><th>u_signup</th></tr>\n",
       "<tr><td>9</td><td>최컴퓨터</td><td>남</td><td>20201124</td></tr>\n",
       "<tr><td>8</td><td>조노트북</td><td>여</td><td>20161201</td></tr>\n",
       "<tr><td>7</td><td>임모바일</td><td>남</td><td>20040807</td></tr>\n",
       "<tr><td>6</td><td>윤디오스</td><td>남</td><td>20040101</td></tr>\n",
       "<tr><td>5</td><td>유코드제로</td><td>여</td><td>20021029</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----+----------+--------+--------+\n",
       "|u_id|    u_name|u_gender|u_signup|\n",
       "+----+----------+--------+--------+\n",
       "|   9|  최컴퓨터|      남|20201124|\n",
       "|   8|  조노트북|      여|20161201|\n",
       "|   7|  임모바일|      남|20040807|\n",
       "|   6|  윤디오스|      남|20040101|\n",
       "|   5|유코드제로|      여|20021029|\n",
       "+----+----------+--------+--------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df9 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"data/tbl_user.csv\")\n",
    ")\n",
    "df9.printSchema()\n",
    "df9.show(10)\n",
    "df9.createOrReplaceTempView(\"tbl_user\")\n",
    "answer = spark.sql(\"select * from tbl_user order by u_signup desc limit 5\")\n",
    "display(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>10. [중급]</font> \"data/tbl_purchase.csv\" 에 저장된 CSV 파일을 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 데이터를 10건 출력하세요\n",
    "#### 3. 상품가격 컬럼(p_amount)을 이용하여 200만원 이상 금액의 상품 가운데 상위 3개를 출력하세요\n",
    "\n",
    "<details><summary>[실습10] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df10 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"data/tbl_purchase.csv\")\n",
    ")\n",
    "df10.printSchema()\n",
    "df10.show(10)\n",
    "df10.createOrReplaceTempView(\"tbl_purchase\")\n",
    "answer = spark.sql(\"select * from tbl_purchase where p_amount > 2000000 order by p_amount desc limit 3\")\n",
    "display(answer)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- p_time: integer (nullable = true)\n",
      " |-- p_uid: integer (nullable = true)\n",
      " |-- p_id: integer (nullable = true)\n",
      " |-- p_name: string (nullable = true)\n",
      " |-- p_amount: integer (nullable = true)\n",
      "\n",
      "+----------+-----+----+-----------+--------+\n",
      "|    p_time|p_uid|p_id|     p_name|p_amount|\n",
      "+----------+-----+----+-----------+--------+\n",
      "|1603651550|    0|1000|GoldStar TV|  100000|\n",
      "|1603651550|    1|2000|    LG DIOS| 2000000|\n",
      "|1603694755|    1|2001|    LG Gram| 1800000|\n",
      "|1603673500|    2|2002|    LG Cyon| 1400000|\n",
      "|1603652155|    3|2003|      LG TV| 1000000|\n",
      "|1603674500|    4|2004|LG Computer| 4500000|\n",
      "|1603665955|    5|2001|    LG Gram| 3500000|\n",
      "|1603666155|    5|2003|      LG TV| 2500000|\n",
      "+----------+-----+----+-----------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>p_time</th><th>p_uid</th><th>p_id</th><th>p_name</th><th>p_amount</th></tr>\n",
       "<tr><td>1603674500</td><td>4</td><td>2004</td><td>LG Computer</td><td>4500000</td></tr>\n",
       "<tr><td>1603665955</td><td>5</td><td>2001</td><td>LG Gram</td><td>3500000</td></tr>\n",
       "<tr><td>1603666155</td><td>5</td><td>2003</td><td>LG TV</td><td>2500000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+-----+----+-----------+--------+\n",
       "|    p_time|p_uid|p_id|     p_name|p_amount|\n",
       "+----------+-----+----+-----------+--------+\n",
       "|1603674500|    4|2004|LG Computer| 4500000|\n",
       "|1603665955|    5|2001|    LG Gram| 3500000|\n",
       "|1603666155|    5|2003|      LG TV| 2500000|\n",
       "+----------+-----+----+-----------+--------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df10 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"data/tbl_purchase.csv\")\n",
    ")\n",
    "df10.printSchema()\n",
    "df10.show(10)\n",
    "df10.createOrReplaceTempView(\"tbl_purchase\")\n",
    "answer = spark.sql(\"select * from tbl_purchase where p_amount > 2000000 order by p_amount desc limit 3\")\n",
    "display(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고자료\n",
    "\n",
    "#### 1. [Spark Programming Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "#### 2. [PySpark SQL Modules Documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)\n",
    "#### 3. <a href=\"https://spark.apache.org/docs/3.0.1/api/sql/\" target=\"_blank\">PySpark 3.0.1 Builtin Functions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
