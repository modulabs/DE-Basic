{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1교시 스파크 기본 명령어\n",
    "\n",
    "> 스파크의 기본 명령어와 구조에 대해 이해합니다\n",
    "\n",
    "## 목차\n",
    "* [1. 스파크를 통한 CSV 파일 읽기](#1.-스파크를-통한-CSV-파일-읽기)\n",
    "* [2. 스파크의 2가지 프로그래밍 방식 비교](#2.-스파크의-2가지-프로그래밍-방식-비교)\n",
    "* [3. 스파크를 통한 JSON 파일 읽기](#3.-스파크를-통한-JSON-파일-읽기)\n",
    "* [4. 뷰 테이블 생성 및 조회](#4.-뷰-테이블-생성-및-조회)\n",
    "* [5. 스파크 애플리케이션의 개념 이해](#5.-스파크-애플리케이션의-개념-이해)\n",
    "* [6. 스파크 UI](#6.-스파크-UI)\n",
    "* [7. M&M 초콜렛 분류 예제](#7.-M&M-초콜렛-분류-예제)\n",
    "* [8. 실습문제](#8.-실습문제)\n",
    "* [참고자료](#참고자료)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 스파크를 통한 CSV 파일 읽기\n",
    "> Spark 3.0.1 버전을 기준으로 작성되었습니다. 스파크는 2.0 버전으로 업데이트 되면서 DataFrames 은 Datasets 으로 통합되었고, 기존의 RDD 에서 사용하던 연산 및 기능과 DataFrame 에서 사용하던 것 모두 사용할 수 있습니다. 스파크 데이터 모델은 RDD (Spark1.0) —> Dataframe(Spark1.3) —> Dataset(Spark1.6) 형태로 업그레이드 되었으나, 본문에서 일부 DataFrames 와 DataSets 가 거의 유사하여, 일부 혼용되어 사용되는 경우가 있을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.6\n",
      "spark.version: 3.0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://9b94a57b6bc9:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f53580f04f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !which python\n",
    "!/opt/conda/bin/python --version\n",
    "\n",
    "print(\"spark.version: {}\".format((spark.version)))\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스파크 사용 관련 팁\n",
    "\n",
    "#### 여러 줄의 코드 작성\n",
    "* python 코드의 경우 괄호로 (python) 묶으면 이스케이핑(\\) 하지 않아도 됩니다\n",
    "* sql 문의 경우  \"\"\"sql\"\"\" 으로 묶으면 이스케이핑(\\)하지 않아도 됩니다\n",
    "\n",
    "#### 데이터 출력 함수\n",
    "* DataFrame.show() - 기본 제공 함수이며, show(n=limit) 통하여 최대 출력을 직접 조정할 수 있으나, 한글 출력 시에 표가 깨지는 경우가 있습니다\n",
    "* display(DataFrame) - Ipython 함수이며, limit 출력을 위해서는 limit 를 걸어야 하지만, 한글 출력에도 깨지지 않습니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- emp_name: string (nullable = true)\n",
      "\n",
      "+------+--------+\n",
      "|emp_id|emp_name|\n",
      "+------+--------+\n",
      "|     1|엘지전자|\n",
      "|     2|엘지화학|\n",
      "+------+--------+\n",
      "\n",
      "+------+\n",
      "|emp_id|\n",
      "+------+\n",
      "|     1|\n",
      "|     2|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>emp_id</th><th>emp_name</th></tr>\n",
       "<tr><td>1</td><td>엘지전자</td></tr>\n",
       "<tr><td>2</td><td>엘지화학</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+--------+\n",
       "|emp_id|emp_name|\n",
       "+------+--------+\n",
       "|     1|엘지전자|\n",
       "|     2|엘지화학|\n",
       "+------+--------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>emp_id</th></tr>\n",
       "<tr><td>1</td></tr>\n",
       "<tr><td>2</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+\n",
       "|emp_id|\n",
       "+------+\n",
       "|     1|\n",
       "|     2|\n",
       "+------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 파이썬 코드 여러 줄 작성\n",
    "json = (\n",
    "    spark\n",
    "    .read\n",
    "    .json(\"data/tmp/simple.json\")\n",
    "    .limit(2)\n",
    ")\n",
    "\n",
    "## 스파크 SQL 여러 줄 작성\n",
    "json.createOrReplaceTempView(\"simple\")\n",
    "spark.sql(\"\"\"\n",
    "    select * \n",
    "    from simple\n",
    "\"\"\")\n",
    "\n",
    "json.printSchema()\n",
    "emp_id = json.select(\"emp_id\")\n",
    "\n",
    "## 표준 데이터 출력함수\n",
    "json.show()\n",
    "emp_id.show()\n",
    "\n",
    "## 노트북 출력함수 \n",
    "display(json)\n",
    "display(emp_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 컨테이너 기반 노트북\n",
    "> 컨테이너 내부에 존재하는 파일 등에 대해 직접 접근이 가능합니다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|value            |\n",
      "+-----------------+\n",
      "|boto3==1.16.32   |\n",
      "|Scrapy==2.4.1    |\n",
      "|selenium==3.141.0|\n",
      "|mrjob==0.7.4     |\n",
      "|pyspark==3.0.1   |\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "count of word is 9\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "strings = spark.read.text(\"../requirements.txt\")\n",
    "strings.show(5, truncate=False)\n",
    "count = strings.count()\n",
    "print(\"count of word is {}\".format(count))\n",
    "\n",
    "strings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.column import Column\n",
    "\n",
    "assert(type(strings) == DataFrame)\n",
    "assert(type(strings.value) == Column) # 현재 strings 데이터프레임의 스키마에 value 라는 하나의 컬럼만 존재합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(strings) # 데이터프레임은 Structured API 를 통해 Row 타입의 레코드를 다루는 함수를 이용하고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(strings.value) # 컬럼은 컬럼과의 비교 혹은 포함된 문자열을 다루는 contains 같은 함수를 사용합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아무런 옵션을 주지 않는 경우 스파크가 알아서 컬럼 이름과 데이터 타입을 (string) 지정합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n",
      "+----------+-----+------+\n",
      "|       _c0|  _c1|   _c2|\n",
      "+----------+-----+------+\n",
      "|    a_time|a_uid|  a_id|\n",
      "|1603645200|    1| login|\n",
      "|1603647200|    1|logout|\n",
      "|1603649200|    2| login|\n",
      "|1603650200|    2|logout|\n",
      "|1603653200|    2| login|\n",
      "|1603657200|    3| login|\n",
      "|1603659200|    3|logout|\n",
      "|1603660200|    4| login|\n",
      "|1603664200|    4|logout|\n",
      "|1603664500|    4| login|\n",
      "|1603666500|    5| login|\n",
      "|1603669500|    5|logout|\n",
      "|1603670500|    6| login|\n",
      "|1603673500|    7| login|\n",
      "|1603674500|    8| login|\n",
      "|1603675500|    9| login|\n",
      "+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_access = spark.read.csv(\"data/log_access.csv\")\n",
    "log_access.printSchema()\n",
    "log_access.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 첫 번째 라인에 헤더가 포함되어 있는 경우 아래와 같이 header option 을 지정하면 컬럼 명을 가져올 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a_time: string (nullable = true)\n",
      " |-- a_uid: string (nullable = true)\n",
      " |-- a_id: string (nullable = true)\n",
      "\n",
      "+----------+-----+------+\n",
      "|    a_time|a_uid|  a_id|\n",
      "+----------+-----+------+\n",
      "|1603645200|    1| login|\n",
      "|1603647200|    1|logout|\n",
      "|1603649200|    2| login|\n",
      "|1603650200|    2|logout|\n",
      "|1603653200|    2| login|\n",
      "|1603657200|    3| login|\n",
      "|1603659200|    3|logout|\n",
      "|1603660200|    4| login|\n",
      "|1603664200|    4|logout|\n",
      "|1603664500|    4| login|\n",
      "|1603666500|    5| login|\n",
      "|1603669500|    5|logout|\n",
      "|1603670500|    6| login|\n",
      "|1603673500|    7| login|\n",
      "|1603674500|    8| login|\n",
      "|1603675500|    9| login|\n",
      "+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_access = spark.read.option(\"header\", \"true\").csv(\"data/log_access.csv\")\n",
    "log_access.printSchema()\n",
    "log_access.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inferSchema 옵션으로 데이터 값을 확인하고 스파크가 데이터 타입을 추정하게 할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a_time: integer (nullable = true)\n",
      " |-- a_uid: integer (nullable = true)\n",
      " |-- a_id: string (nullable = true)\n",
      "\n",
      "+----------+-----+------+\n",
      "|    a_time|a_uid|  a_id|\n",
      "+----------+-----+------+\n",
      "|1603645200|    1| login|\n",
      "|1603647200|    1|logout|\n",
      "|1603649200|    2| login|\n",
      "|1603650200|    2|logout|\n",
      "|1603653200|    2| login|\n",
      "|1603657200|    3| login|\n",
      "|1603659200|    3|logout|\n",
      "|1603660200|    4| login|\n",
      "|1603664200|    4|logout|\n",
      "|1603664500|    4| login|\n",
      "|1603666500|    5| login|\n",
      "|1603669500|    5|logout|\n",
      "|1603670500|    6| login|\n",
      "|1603673500|    7| login|\n",
      "|1603674500|    8| login|\n",
      "|1603675500|    9| login|\n",
      "+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_access = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"data/log_access.csv\")\n",
    "log_access.printSchema()\n",
    "log_access.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>1. [기본]</font> \"data/flight-data/csv/2010-summary.csv\" 파일의 스키마와 데이터 10건을 출력하세요\n",
    "\n",
    "<details><summary>[정답] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df1 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"data/flight-data/csv/2010-summary.csv\")\n",
    ")\n",
    "    \n",
    "df1.printSchema()\n",
    "df1.show(10)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "|    United States|          Singapore|   25|\n",
      "|    United States|            Grenada|   54|\n",
      "|       Costa Rica|      United States|  477|\n",
      "|          Senegal|      United States|   29|\n",
      "|    United States|   Marshall Islands|   44|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df1 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"data/flight-data/csv/2010-summary.csv\")\n",
    ")\n",
    "\n",
    "df1.printSchema()\n",
    "df1.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 스파크의 2가지 프로그래밍 방식 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하나. 구조화된 API 호출을 통해 데이터를 출력하는 방법\n",
    "> 출력 시에 bigint 값인 날짜는 아래와 같이 from_unixtime 및 to_timestamp 함수를 통해 변환할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+\n",
      "| Arrival_Time|    String_Datetime|\n",
      "+-------------+-------------------+\n",
      "|1424686734992|2015-02-23 19:18:54|\n",
      "|1424686735190|2015-02-23 19:18:55|\n",
      "|1424686735395|2015-02-23 19:18:55|\n",
      "|1424686735593|2015-02-23 19:18:55|\n",
      "|1424686735795|2015-02-23 19:18:55|\n",
      "+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, to_timestamp, to_date, col, lit\n",
    "\n",
    "df = spark.read.option(\"inferSchema\", \"true\").json(\"data/activity-data\")\n",
    "\n",
    "# 구조화된 API 를 통한 구문\n",
    "timestamp = df.select(\n",
    "    \"Arrival_Time\",\n",
    "    to_timestamp(from_unixtime(col('Arrival_Time') / lit(1000)), 'yyyy-MM-dd HH:mm:ss').alias('String_Datetime')\n",
    ")\n",
    "timestamp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 둘. 표현식 형식으로 그대로 사용하여 출력하는 방법\n",
    "> 컬럼(col) 혹은 함수(concat 등)를 직접 사용하는 방식을 **구조화된 API** 를 사용한다고 말하고 SQL 구문으로 표현하는 방식을 **SQL 표현식**을 사용한다고 말합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+\n",
      "| Arrival_Time|    String_Datetime|\n",
      "+-------------+-------------------+\n",
      "|1424686734992|2015-02-23 19:18:54|\n",
      "|1424686735190|2015-02-23 19:18:55|\n",
      "|1424686735395|2015-02-23 19:18:55|\n",
      "|1424686735593|2015-02-23 19:18:55|\n",
      "|1424686735795|2015-02-23 19:18:55|\n",
      "+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL Expression 통한 구문\n",
    "ts = df.selectExpr(\n",
    "    \"Arrival_Time\",\n",
    "    \"to_timestamp(from_unixtime(Arrival_Time / 1000), 'yyyy-MM-dd HH:mm:ss') as String_Datetime\"\n",
    ")\n",
    "ts.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>2. [기본]</font> \"data/activity-data\" 경로에 저장된 json 파일을 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 10건의 데이터를 출력하세요\n",
    "#### 3. 'Creation_Time' 컬럼을 년월일 포맷으로 'String_Creation_Date' 컬럼을 출력하세요\n",
    "> 단, Creation_Time 은 Arrival_Time 과 정밀도가 달라서 1000 이 아니라 `1000000000` 을 나누어 주어야 합니다\n",
    "\n",
    "<details><summary>[실습2] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "```python\n",
    "df2 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(\"data/activity-data\")\n",
    ")\n",
    "    \n",
    "df2.printSchema()\n",
    "display(df2.limit(3))\n",
    "answer = df2.limit(3).selectExpr(\n",
    "    \"Creation_Time\",\n",
    "    \"to_timestamp(from_unixtime(Creation_Time / 1000000000), 'yyyy-MM-dd HH:mm:ss') as String_Creation_Date\"\n",
    ")\n",
    "answer.show(10)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Arrival_Time</th><th>Creation_Time</th><th>Device</th><th>Index</th><th>Model</th><th>User</th><th>gt</th><th>x</th><th>y</th><th>z</th></tr>\n",
       "<tr><td>1424686734992</td><td>1424688581040070924</td><td>nexus4_2</td><td>5</td><td>nexus4</td><td>g</td><td>stand</td><td>-3.814697E-4</td><td>0.025878906</td><td>0.023727417</td></tr>\n",
       "<tr><td>1424686735190</td><td>1424688581245179566</td><td>nexus4_2</td><td>46</td><td>nexus4</td><td>g</td><td>stand</td><td>-0.008926392</td><td>-0.047821045</td><td>0.011978149</td></tr>\n",
       "<tr><td>1424686735395</td><td>1424686733397706064</td><td>nexus4_1</td><td>79</td><td>nexus4</td><td>g</td><td>stand</td><td>3.356934E-4</td><td>0.02507019</td><td>-0.005996704</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
       "| Arrival_Time|      Creation_Time|  Device|Index| Model|User|   gt|           x|           y|           z|\n",
       "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
       "|1424686734992|1424688581040070924|nexus4_2|    5|nexus4|   g|stand|-3.814697E-4| 0.025878906| 0.023727417|\n",
       "|1424686735190|1424688581245179566|nexus4_2|   46|nexus4|   g|stand|-0.008926392|-0.047821045| 0.011978149|\n",
       "|1424686735395|1424686733397706064|nexus4_1|   79|nexus4|   g|stand| 3.356934E-4|  0.02507019|-0.005996704|\n",
       "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|      Creation_Time|String_Creation_Date|\n",
      "+-------------------+--------------------+\n",
      "|1424688581040070924| 2015-02-23 19:49:41|\n",
      "|1424688581245179566| 2015-02-23 19:49:41|\n",
      "|1424686733397706064| 2015-02-23 19:18:53|\n",
      "+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df2 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(\"data/activity-data\")\n",
    ")\n",
    "\n",
    "df2.printSchema()\n",
    "display(df2.limit(3))\n",
    "answer = df2.limit(3).selectExpr(\n",
    "    \"Creation_Time\",\n",
    "    \"to_timestamp(from_unixtime(Creation_Time / 1000000000), 'yyyy-MM-dd HH:mm:ss') as String_Creation_Date\"\n",
    ")\n",
    "answer.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select 뿐만 아니라 filter 의 경우도 Expression 을 사용할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|count|\n",
      "+----+-----+\n",
      "|   g|91650|\n",
      "|   f|92030|\n",
      "|   e|96000|\n",
      "|   h|77300|\n",
      "|   d|81220|\n",
      "|   c|77130|\n",
      "|   i|92530|\n",
      "|   b|91210|\n",
      "|   a|80824|\n",
      "+----+-----+\n",
      "\n",
      "+----+-----+\n",
      "|user|count|\n",
      "+----+-----+\n",
      "|   g|91650|\n",
      "|   f|92030|\n",
      "|   e|96000|\n",
      "|   h|77300|\n",
      "|   d|81220|\n",
      "|   c|77130|\n",
      "|   i|92530|\n",
      "|   b|91210|\n",
      "|   a|80824|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"index\") > 100).select(\"index\", \"user\").groupBy(\"user\").count().show()\n",
    "# 대부분의 구문에서 표현식을 통해 처리할 수 있도록 내부적으로 2가지 방식에 대해 모두 구현되어 있습니다. \n",
    "df.filter(\"index > 100\").select(\"index\", \"user\").groupBy(\"user\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 스파크를 통한 JSON 파일 읽기\n",
    "\n",
    "> 추후에 학습하게 될 예정인 filter 및 groupBy 구문이 사용되고 있는데요, 조건을 통해 데이터를 줄이고(filter), 특정 컬럼별 집계(groupBy)를 위한 연산자입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|count|\n",
      "+----+-----+\n",
      "|   g|91650|\n",
      "|   f|92030|\n",
      "|   e|96000|\n",
      "|   h|77300|\n",
      "|   d|81220|\n",
      "+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json = spark.read.json(\"data/activity-data\")\n",
    "users = json.filter(\"index > 100\").select(\"index\", \"user\").groupBy(\"user\").count()\n",
    "users.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>3. [기본]</font> \"data/activity-data\" 경로의 JSON 데이터를 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. 10건의 데이터를 출력하세요\n",
    "#### 3. index 가 10000 미만의 이용자('user')별 빈도수를 구하세요\n",
    "\n",
    "<details><summary>[실습3] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "df3 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(\"data/activity-data\")\n",
    ")\n",
    "    \n",
    "df3.printSchema()\n",
    "answer = df3.filter(\"index < 10000\").groupBy(\"user\").count()\n",
    "answer.show(10)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n",
      "+----+-----+\n",
      "|user|count|\n",
      "+----+-----+\n",
      "|   g| 2506|\n",
      "|   f| 2490|\n",
      "|   e| 2501|\n",
      "|   h| 2500|\n",
      "|   d| 2499|\n",
      "|   c| 2494|\n",
      "|   i| 2500|\n",
      "|   b| 2500|\n",
      "|   a| 2501|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df3 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(\"data/activity-data\")\n",
    ")\n",
    "\n",
    "df3.printSchema()\n",
    "answer = df3.filter(\"index < 10000\").groupBy(\"user\").count()\n",
    "answer.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 뷰 테이블 생성 및 조회\n",
    "> 이미 생성된 데이터 프레임을 통해서 현재 세션에서만 조회 가능한 임시 뷰 테이블을 만들어 SQL 질의가 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|count|\n",
      "+----+-----+\n",
      "|   e|96000|\n",
      "|   i|92530|\n",
      "|   f|92030|\n",
      "|   g|91650|\n",
      "|   b|91210|\n",
      "+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.createOrReplaceTempView(\"users\")\n",
    "spark.sql(\"select * from users where count is not null and count > 9000 order by count desc\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>4. [기본]</font> \"data/flight-data/json/2015-summary.json\" 경로의 JSON 데이터를 읽고\n",
    "#### 1. `2015_summary` 라는 임시 테이블을 생성하세요\n",
    "#### 2. spark sql 구문을 이용하여 10 건의 데이터를 출력하세요\n",
    "\n",
    "<details><summary>[실습4] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "df4 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(\"data/flight-data/json/2015-summary.json\")\n",
    ")\n",
    "    \n",
    "df4.printSchema()\n",
    "answer = df4.createOrReplaceTempView(\"2015_summary\")\n",
    "spark.sql(\"select * from 2015_summary\").show(10)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df4 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(\"data/flight-data/json/2015-summary.json\")\n",
    ")\n",
    "\n",
    "df4.printSchema()\n",
    "answer = df4.createOrReplaceTempView(\"2015_summary\")\n",
    "spark.sql(\"select * from 2015_summary\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON 파일을 읽는 여러가지 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 스키마 확인 - 3가지 모두 동일한 결과를 얻을 수 있으며 편한 방식을 선택하시면 됩니다\n",
    "df = spark.read.format(\"json\").load(\"./data/flight-data/json/2015-summary.json\") # 미국 교통통계국이 제공하는 항공운항 데이터\n",
    "df.printSchema()\n",
    "\n",
    "df2 = spark.read.load(\"./data/flight-data/json/2015-summary.json\", format=\"json\")\n",
    "df2.printSchema()\n",
    "\n",
    "df3 = spark.read.json(\"./data/flight-data/json/2015-summary.json\")\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 스파크 애플리케이션의 개념 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스파크에서 반드시 알아야 할 객체와 개념\n",
    "| 구분 | 설명 | 기타 |\n",
    "|---|---|---|\n",
    "| Application | 스파크 프레임워크를 통해 빌드한 프로그램. 전체 작업을 관리하는 Driver 와 Executors 상에서 수행되는 프로그램으로 구분합니다 | - |\n",
    "| SparkSession | 스파크의 모든 기능을 사용하기 위해 생성하는 객체 | - |\n",
    "| Job | 하나의 액션(save, collect 등)을 수행하기 위해 여러개의 타스크로 구성된 병렬처리 단위 | DAG 혹은 Spark Execution Plan |\n",
    "| Stage | 하나의 잡은 다수의 스테이지라는 것으로 구성되며, 하나의 스테이지는 다수의 타스크 들로 구성됩니다 | - |\n",
    "| Task | 스파크 익스큐터에 보내지는 하나의 작업 단위 | 하나의 Core 혹은 Partition 단위의 작업 |\n",
    "\n",
    "### 스파크의 변환(Transformation)과 액션(Action)\n",
    "| 구분 | 설명 | 기타 |\n",
    "|---|---|---|\n",
    "| Transformation | 원본 데이터의 변경 없이 새로운 데이터프레임을 생성하는 모든 작업을 말하며 모든 변환 작업들은 lazily evaluated 되며 lineage 를 유지합니다 | select, filter, join, groupBy, orderBy |\n",
    "| Action | 여태까지 지연된 변환 작업을 트리거링하는 동작을 말하며, 데이터의 조회 및 저장 등의 작업을 말합니다 | show, take, count, collect, save |\n",
    "\n",
    "> Lineage : 연속된 변환 파이프라인이 액션을 만나기 전까지의 모든 이력 혹은 히스토리 정보를 모두 저장하고 있는 객체를 말하며, 스파크는 이렇게 체인을 구성한 변환 작업을 통해 **쿼리 최적화**를 수행할 수 있으며, 데이터 불변성(Immutability)를 통해서 **내결함성(Fault Tolerance)**을 가질 수 있습니다.\n",
    "\n",
    "### 좁은 변환과 넓은 변환 - Narrow and Wide Transformation\n",
    "> 위에서 언급한 최적화(Optimization) 과정은 여러 오퍼레이션들을 다시 여러 스테이지로 쪼개고, 이러한 스테이지 들이 셔플링이 필요한지, 클러스터간의 데이터 교환이 필요한지 등을 결정하는 문제이며, 변환 작업은 크게 하나의 파티션 내에 수행이 가능한 **좁은 의존성(narrow dependencies)** 과 셔플링이 발생하여 클러스터 전체에 데이터 교환이 필요한 **넓은 의존성(wide dependencies)** 두 가지로 구분합니다\n",
    "\n",
    "![Transformation](images/transformation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 스파크 UI\n",
    "> Default 포트는 4040 이므로 http://localhost:4040 에 접속하여 앞에서 배웠던 Narrow, Wide Transformation DAG를 확인합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|value                                   |\n",
      "+----------------------------------------+\n",
      "|jupyter-contrib-nbextensions==0.5.1     |\n",
      "|jupyter-nbextensions-configurator==0.4.1|\n",
      "+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Narrow Transformation\n",
    "strings = spark.read.text(\"../requirements.txt\")\n",
    "jupyter = strings.filter(strings.value.contains(\"jupyter\"))\n",
    "jupyter.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|u_gender|count|\n",
      "+--------+-----+\n",
      "|여      |3    |\n",
      "|남      |6    |\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Wide Transformation\n",
    "user = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"data/tbl_user.csv\")\n",
    "count = user.groupBy(\"u_gender\").count()\n",
    "count.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Narrow | Wide |\n",
    "|---|---|\n",
    "|![narrow](images/narrow.png)|![wide](images/wide.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. M&M 초콜렛 분류 예제 (참고용)\n",
    "> [Learning Spark 2nd Edition](https://github.com/psyoblade/LearningSparkV2?organization=psyoblade&organization=psyoblade) 에서 제공하는 data bricks dataset 예제 가운데 미국의 주 별 M&M 초콜렛 판매량을 집계하는 예제를 작성합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- State: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n",
      "+-----+------+-----+\n",
      "|State|Color |Count|\n",
      "+-----+------+-----+\n",
      "|TX   |Red   |20   |\n",
      "|NV   |Blue  |66   |\n",
      "|CO   |Blue  |79   |\n",
      "|OR   |Blue  |71   |\n",
      "|WA   |Yellow|93   |\n",
      "|WY   |Blue  |16   |\n",
      "|CA   |Yellow|53   |\n",
      "|WA   |Green |60   |\n",
      "|OR   |Green |71   |\n",
      "|TX   |Green |68   |\n",
      "|NV   |Green |59   |\n",
      "|AZ   |Brown |95   |\n",
      "|WA   |Yellow|20   |\n",
      "|AZ   |Blue  |75   |\n",
      "|OR   |Brown |72   |\n",
      "|NV   |Red   |98   |\n",
      "|WY   |Orange|45   |\n",
      "|CO   |Blue  |52   |\n",
      "|TX   |Brown |94   |\n",
      "|CO   |Red   |82   |\n",
      "+-----+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnm_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"data/databricks/mnm_dataset.csv\")\n",
    "mnm_df.printSchema()\n",
    "mnm_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|State|Color |Total|\n",
      "+-----+------+-----+\n",
      "|CA   |Yellow|1807 |\n",
      "|WA   |Green |1779 |\n",
      "|OR   |Orange|1743 |\n",
      "|TX   |Green |1737 |\n",
      "|TX   |Red   |1725 |\n",
      "|CA   |Green |1723 |\n",
      "|CO   |Yellow|1721 |\n",
      "|CA   |Brown |1718 |\n",
      "|CO   |Green |1713 |\n",
      "|NV   |Orange|1712 |\n",
      "|TX   |Yellow|1703 |\n",
      "|NV   |Green |1698 |\n",
      "|AZ   |Brown |1698 |\n",
      "|CO   |Blue  |1695 |\n",
      "|WY   |Green |1695 |\n",
      "|NM   |Red   |1690 |\n",
      "|AZ   |Orange|1689 |\n",
      "|NM   |Yellow|1688 |\n",
      "|NM   |Brown |1687 |\n",
      "|UT   |Orange|1684 |\n",
      "|NM   |Green |1682 |\n",
      "|UT   |Red   |1680 |\n",
      "|AZ   |Green |1676 |\n",
      "|NV   |Yellow|1675 |\n",
      "|NV   |Blue  |1673 |\n",
      "|WA   |Red   |1671 |\n",
      "|WY   |Red   |1670 |\n",
      "|WA   |Brown |1669 |\n",
      "|NM   |Orange|1665 |\n",
      "|WY   |Blue  |1664 |\n",
      "|WA   |Yellow|1663 |\n",
      "|WA   |Orange|1658 |\n",
      "|CA   |Orange|1657 |\n",
      "|NV   |Brown |1657 |\n",
      "|CO   |Brown |1656 |\n",
      "|CA   |Red   |1656 |\n",
      "|UT   |Blue  |1655 |\n",
      "|AZ   |Yellow|1654 |\n",
      "|TX   |Orange|1652 |\n",
      "|AZ   |Red   |1648 |\n",
      "|OR   |Blue  |1646 |\n",
      "|UT   |Yellow|1645 |\n",
      "|OR   |Red   |1645 |\n",
      "|CO   |Orange|1642 |\n",
      "|TX   |Brown |1641 |\n",
      "|NM   |Blue  |1638 |\n",
      "|AZ   |Blue  |1636 |\n",
      "|OR   |Green |1634 |\n",
      "|UT   |Brown |1631 |\n",
      "|WY   |Yellow|1626 |\n",
      "|WA   |Blue  |1625 |\n",
      "|CO   |Red   |1624 |\n",
      "|OR   |Brown |1621 |\n",
      "|TX   |Blue  |1614 |\n",
      "|OR   |Yellow|1614 |\n",
      "|NV   |Red   |1610 |\n",
      "|CA   |Blue  |1603 |\n",
      "|WY   |Orange|1595 |\n",
      "|UT   |Green |1591 |\n",
      "|WY   |Brown |1532 |\n",
      "+-----+------+-----+\n",
      "\n",
      "Total Rows = 60\n",
      "+-----+------+-----+\n",
      "|State|Color |Total|\n",
      "+-----+------+-----+\n",
      "|CA   |Yellow|1807 |\n",
      "|CA   |Green |1723 |\n",
      "|CA   |Brown |1718 |\n",
      "|CA   |Orange|1657 |\n",
      "|CA   |Red   |1656 |\n",
      "|CA   |Blue  |1603 |\n",
      "+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# We use the DataFrame high-level APIs. Note\n",
    "# that we don't use RDDs at all. Because some of Spark's\n",
    "# functions return the same object, we can chain function calls.\n",
    "# 1. Select from the DataFrame the fields \"State\", \"Color\", and \"Count\"\n",
    "# 2. Since we want to group each state and its M&M color count,\n",
    "# we use groupBy()\n",
    "# 3. Aggregate counts of all colors and groupBy() State and Color\n",
    "# 4 orderBy() in descending order\n",
    "count_mnm_df = (mnm_df.select(\"State\", \"Color\", \"Count\") \\\n",
    ".groupBy(\"State\", \"Color\") \\\n",
    ".agg(count(\"Count\").alias(\"Total\")) \\\n",
    ".orderBy(\"Total\", ascending=False))\n",
    "# Show the resulting aggregations for all the states and colors;\n",
    "# a total count of each color per state.\n",
    "# Note show() is an action, which will trigger the above\n",
    "# query to be executed.\n",
    "count_mnm_df.show(n=60, truncate=False)\n",
    "print(\"Total Rows = %d\" % (count_mnm_df.count()))\n",
    "\n",
    "# While the above code aggregated and counted for all\n",
    "# the states, what if we just want to see the data for\n",
    "# a single state, e.g., CA?\n",
    "# 1. Select from all rows in the DataFrame\n",
    "# 2. Filter only CA state\n",
    "# 3. groupBy() State and Color as we did above\n",
    "# 4. Aggregate the counts for each color\n",
    "# 5. orderBy() in descending order\n",
    "# Find the aggregate count for California by filtering\n",
    "ca_count_mnm_df = (mnm_df.select(\"State\", \"Color\", \"Count\") \\\n",
    ".where(mnm_df.State == \"CA\") \\\n",
    ".groupBy(\"State\", \"Color\") \\\n",
    ".agg(count(\"Count\").alias(\"Total\")) \\\n",
    ".orderBy(\"Total\", ascending=False))\n",
    "# Show the resulting aggregation for California.\n",
    "# As above, show() is an action that will trigger the execution of the\n",
    "# entire computation.\n",
    "ca_count_mnm_df.show(n=10, truncate=False)\n",
    "# Stop the SparkSession\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>5. [기본]</font> \"data/tbl_user.csv\" 경로의 CSV 데이터를 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. `user` 라는 임시 테이블을 생성하세요\n",
    "#### 3. spark sql 구문을 이용하여 10 건의 데이터를 출력하세요\n",
    "\n",
    "<details><summary>[실습5] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "df5 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"data/tbl_user.csv\")\n",
    ")\n",
    "    \n",
    "df5.printSchema()\n",
    "answer = df5.createOrReplaceTempView(\"user\")\n",
    "spark.sql(\"select * from user\").show(10)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- u_id: integer (nullable = true)\n",
      " |-- u_name: string (nullable = true)\n",
      " |-- u_gender: string (nullable = true)\n",
      " |-- u_signup: integer (nullable = true)\n",
      "\n",
      "+----+----------+--------+--------+\n",
      "|u_id|    u_name|u_gender|u_signup|\n",
      "+----+----------+--------+--------+\n",
      "|   1|    정휘센|      남|19700808|\n",
      "|   2|  김싸이언|      남|19710201|\n",
      "|   3|    박트롬|      여|19951030|\n",
      "|   4|    청소기|      남|19770329|\n",
      "|   5|유코드제로|      여|20021029|\n",
      "|   6|  윤디오스|      남|20040101|\n",
      "|   7|  임모바일|      남|20040807|\n",
      "|   8|  조노트북|      여|20161201|\n",
      "|   9|  최컴퓨터|      남|20201124|\n",
      "+----+----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df5 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"data/tbl_user.csv\")\n",
    ")\n",
    "\n",
    "df5.printSchema()\n",
    "answer = df5.createOrReplaceTempView(\"user\")\n",
    "spark.sql(\"select * from user\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>6. [기본]</font> \"data/tbl_purchase.csv\" 경로의 CSV 데이터를 읽고\n",
    "#### 1. 스키마를 출력하세요\n",
    "#### 2. `purchase` 라는 임시 테이블을 생성하세요\n",
    "#### 3. selectExpr 구문 혹은 spark sql 구문을 이용하여 `p_time` 필드를 날짜 함수를 이용하여 식별 가능하도록 데이터를 출력하세요\n",
    "\n",
    "<details><summary>[실습6] 출력 결과 확인 </summary>\n",
    "\n",
    "> 아래와 유사하게 방식으로 작성 되었다면 정답입니다\n",
    "\n",
    "\n",
    "```python\n",
    "df6 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"data/tbl_purchase.csv\")\n",
    ")\n",
    "    \n",
    "df6.printSchema()\n",
    "answer = df6.createOrReplaceTempView(\"purchase\")\n",
    "spark.sql(\"select from_unixtime(p_time) as p_time from purchase\").show(10)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- p_time: integer (nullable = true)\n",
      " |-- p_uid: integer (nullable = true)\n",
      " |-- p_id: integer (nullable = true)\n",
      " |-- p_name: string (nullable = true)\n",
      " |-- p_amount: integer (nullable = true)\n",
      "\n",
      "+-------------------+\n",
      "|             p_time|\n",
      "+-------------------+\n",
      "|2020-10-26 03:45:50|\n",
      "|2020-10-26 03:45:50|\n",
      "|2020-10-26 15:45:55|\n",
      "|2020-10-26 09:51:40|\n",
      "|2020-10-26 03:55:55|\n",
      "|2020-10-26 10:08:20|\n",
      "|2020-10-26 07:45:55|\n",
      "|2020-10-26 07:49:15|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기에 실습 코드를 작성하고 실행하세요 (Shift+Enter)\n",
    "df6 = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"data/tbl_purchase.csv\")\n",
    ")\n",
    "\n",
    "df6.printSchema()\n",
    "answer = df6.createOrReplaceTempView(\"purchase\")\n",
    "spark.sql(\"select from_unixtime(p_time) as p_time from purchase\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고자료\n",
    "\n",
    "#### 1. [Spark Programming Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "#### 2. [PySpark SQL Modules Documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)\n",
    "#### 3. <a href=\"https://spark.apache.org/docs/3.0.1/api/sql/\" target=\"_blank\">PySpark 3.0.1 Builtin Functions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
