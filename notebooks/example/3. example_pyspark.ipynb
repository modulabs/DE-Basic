{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "485705aa-6073-4087-bbb7-8dd646b59cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://8fe632226952:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f137c0f0460>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7957d4e-ebf1-46b0-aa83-fdce6dce44e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.sql.session.timeZone', 'Asia/Seoul')\n",
      "('spark.driver.port', '34951')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.app.name', 'pyspark-shell')\n",
      "('spark.app.id', 'local-1653614399786')\n",
      "('spark.driver.extraJavaOptions', '-Dio.netty.tryReflectionSetAccessible=true')\n",
      "('spark.app.startTime', '1653614398910')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.sql.warehouse.dir', 'file:/home/jovyan/work/example/spark-warehouse')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.driver.host', '8fe632226952')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.executor.extraJavaOptions', '-Dio.netty.tryReflectionSetAccessible=true')\n",
      "('spark.ui.showConsoleProgress', 'true')\n"
     ]
    }
   ],
   "source": [
    "# dictionary = spark.conf.get(\"parquet.enable.dictionary\")\n",
    "from pyspark.context import SparkContext as sc\n",
    "\n",
    "for conf in spark.sparkContext.getConf().getAll():\n",
    "    print(conf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3bb2199-defd-42b5-8ac7-0cca20343437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+-----+\n",
      "|       source|         target|visit|\n",
      "+-------------+---------------+-----+\n",
      "|  New Country|  Other Country|    5|\n",
      "|New Country 2|Other Country 3|    1|\n",
      "+-------------+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" union 함수 \"\"\"\n",
    "from pyspark.sql import Row\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"source\", StringType(), False),\n",
    "    StructField(\"target\", StringType(), False),\n",
    "    StructField(\"visit\", IntegerType(), False),\n",
    "])\n",
    "newRows = [\n",
    "    Row(\"New Country\", \"Other Country\", 5),\n",
    "    Row(\"New Country 2\", \"Other Country 3\", 1)\n",
    "]\n",
    "\n",
    "# Parallelized Collections :\n",
    "# Parallelized collections are created by calling SparkContext’s parallelize method on an existing iterable or collection in your driver program.\n",
    "# The elements of the collection are copied to form a distributed dataset that can be operated on in parallel. \n",
    "\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows) \n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)\n",
    "\n",
    "newDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f510e716-1db3-4ded-a72e-9b626fcfa770",
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF.write.mode(\"overwrite\").parquet(\"foo/parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1baa5515-ec5e-43f3-a01f-836085579a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- source: string (nullable = true)\n",
      " |-- target: string (nullable = true)\n",
      " |-- visit: integer (nullable = true)\n",
      "\n",
      "+-------------+---------------+-----+\n",
      "|source       |target         |visit|\n",
      "+-------------+---------------+-----+\n",
      "|New Country 2|Other Country 3|1    |\n",
      "|New Country  |Other Country  |5    |\n",
      "+-------------+---------------+-----+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "Relation [source#74,target#75,visit#76] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "source: string, target: string, visit: int\n",
      "Relation [source#74,target#75,visit#76] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation [source#74,target#75,visit#76] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [source#74,target#75,visit#76] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/work/foo/parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<source:string,target:string,visit:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo = spark.read.parquet(\"foo/parquet\")\n",
    "foo.printSchema()\n",
    "foo.show(10, truncate=False)\n",
    "foo.explain(mode='extended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84db3d6b-dfb1-428b-8e89-67a57d91f593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['visit]\n",
      "+- Relation [source#48,target#49,visit#50] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "visit: int\n",
      "Project [visit#50]\n",
      "+- Relation [source#48,target#49,visit#50] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [visit#50]\n",
      "+- Relation [source#48,target#49,visit#50] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [visit#50] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/work/foo/parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<visit:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_pruning = spark.read.parquet(\"foo/parquet\")\n",
    "column_pruning.select(\"visit\").explain(mode='extended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2bebe26-e3a5-4f02-93db-f6e8ee20b5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('visit > 100)\n",
      "+- Relation [source#93,target#94,visit#95] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "source: string, target: string, visit: int\n",
      "Filter (visit#95 > 100)\n",
      "+- Relation [source#93,target#94,visit#95] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(visit#95) AND (visit#95 > 100))\n",
      "+- Relation [source#93,target#94,visit#95] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(visit#95) AND (visit#95 > 100))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [source#93,target#94,visit#95] Batched: true, DataFilters: [isnotnull(visit#95), (visit#95 > 100)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/work/foo/parquet], PartitionFilters: [], PushedFilters: [IsNotNull(visit), GreaterThan(visit,100)], ReadSchema: struct<source:string,target:string,visit:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_pushdown = spark.read.parquet(\"foo/parquet\")\n",
    "filter_pushdown.where(\"visit > 100\").explain(mode='extended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aaba38-e109-4d57-9b91-f2a1612cd458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
