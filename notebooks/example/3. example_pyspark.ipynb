{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "485705aa-6073-4087-bbb7-8dd646b59cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://fdd1c12be7c4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb874198220>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7957d4e-ebf1-46b0-aa83-fdce6dce44e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.sql.session.timeZone', 'Asia/Seoul')\n",
      "('spark.driver.port', '34951')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.app.name', 'pyspark-shell')\n",
      "('spark.app.id', 'local-1653614399786')\n",
      "('spark.driver.extraJavaOptions', '-Dio.netty.tryReflectionSetAccessible=true')\n",
      "('spark.app.startTime', '1653614398910')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.sql.warehouse.dir', 'file:/home/jovyan/work/example/spark-warehouse')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.driver.host', '8fe632226952')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.executor.extraJavaOptions', '-Dio.netty.tryReflectionSetAccessible=true')\n",
      "('spark.ui.showConsoleProgress', 'true')\n"
     ]
    }
   ],
   "source": [
    "# dictionary = spark.conf.get(\"parquet.enable.dictionary\")\n",
    "from pyspark.context import SparkContext as sc\n",
    "\n",
    "for conf in spark.sparkContext.getConf().getAll():\n",
    "    print(conf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3bb2199-defd-42b5-8ac7-0cca20343437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+-----+\n",
      "|       source|         target|visit|\n",
      "+-------------+---------------+-----+\n",
      "|  New Country|  Other Country|    5|\n",
      "|New Country 2|Other Country 3|    1|\n",
      "+-------------+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" union 함수 \"\"\"\n",
    "from pyspark.sql import Row\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"source\", StringType(), False),\n",
    "    StructField(\"target\", StringType(), False),\n",
    "    StructField(\"visit\", IntegerType(), False),\n",
    "])\n",
    "newRows = [\n",
    "    Row(\"New Country\", \"Other Country\", 5),\n",
    "    Row(\"New Country 2\", \"Other Country 3\", 1)\n",
    "]\n",
    "\n",
    "# Parallelized Collections :\n",
    "# Parallelized collections are created by calling SparkContext’s parallelize method on an existing iterable or collection in your driver program.\n",
    "# The elements of the collection are copied to form a distributed dataset that can be operated on in parallel. \n",
    "\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows) \n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)\n",
    "\n",
    "newDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f510e716-1db3-4ded-a72e-9b626fcfa770",
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF.write.mode(\"overwrite\").parquet(\"foo/parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1baa5515-ec5e-43f3-a01f-836085579a04",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- source: string (nullable = true)\n",
      " |-- target: string (nullable = true)\n",
      " |-- visit: integer (nullable = true)\n",
      "\n",
      "+-------------+---------------+-----+\n",
      "|source       |target         |visit|\n",
      "+-------------+---------------+-----+\n",
      "|New Country 2|Other Country 3|1    |\n",
      "|New Country  |Other Country  |5    |\n",
      "+-------------+---------------+-----+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "Relation [source#74,target#75,visit#76] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "source: string, target: string, visit: int\n",
      "Relation [source#74,target#75,visit#76] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation [source#74,target#75,visit#76] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [source#74,target#75,visit#76] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/work/foo/parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<source:string,target:string,visit:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo = spark.read.parquet(\"foo/parquet\")\n",
    "foo.printSchema()\n",
    "foo.show(10, truncate=False)\n",
    "foo.explain(mode='extended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84db3d6b-dfb1-428b-8e89-67a57d91f593",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['visit]\n",
      "+- Relation [source#48,target#49,visit#50] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "visit: int\n",
      "Project [visit#50]\n",
      "+- Relation [source#48,target#49,visit#50] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [visit#50]\n",
      "+- Relation [source#48,target#49,visit#50] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [visit#50] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/work/foo/parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<visit:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_pruning = spark.read.parquet(\"foo/parquet\")\n",
    "column_pruning.select(\"visit\").explain(mode='extended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2bebe26-e3a5-4f02-93db-f6e8ee20b5ef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('visit > 100)\n",
      "+- Relation [source#93,target#94,visit#95] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "source: string, target: string, visit: int\n",
      "Filter (visit#95 > 100)\n",
      "+- Relation [source#93,target#94,visit#95] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(visit#95) AND (visit#95 > 100))\n",
      "+- Relation [source#93,target#94,visit#95] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(visit#95) AND (visit#95 > 100))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [source#93,target#94,visit#95] Batched: true, DataFilters: [isnotnull(visit#95), (visit#95 > 100)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/work/foo/parquet], PartitionFilters: [], PushedFilters: [IsNotNull(visit), GreaterThan(visit,100)], ReadSchema: struct<source:string,target:string,visit:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_pushdown = spark.read.parquet(\"foo/parquet\")\n",
    "filter_pushdown.where(\"visit > 100\").explain(mode='extended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7aaba38-e109-4d57-9b91-f2a1612cd458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- title_eng: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies = spark.read.option(\"header\", \"true\").option(\"delimiter\", \"\\t\").option(\"inferSchema\", \"true\").csv(\"/home/jovyan/work/data/kmrd-small/movies.txt\")\n",
    "movies.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6e1c148-1a1d-42ed-b59d-391f9451a1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- people: integer (nullable = true)\n",
      " |-- korean: string (nullable = true)\n",
      " |-- original: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peoples = spark.read.option(\"header\", \"true\").option(\"delimiter\", \"\\t\").option(\"inferSchema\", \"true\").csv(\"/home/jovyan/work/data/kmrd-small/peoples.txt\")\n",
    "peoples.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20a2b0b0-dbcb-4fb0-9706-d240f8ae084e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie: integer (nullable = true)\n",
      " |-- people: integer (nullable = true)\n",
      " |-- order: integer (nullable = true)\n",
      " |-- leading: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "castings = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/home/jovyan/work/data/kmrd-small/castings.csv\")\n",
    "castings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab90e7f2-c951-45ef-82ae-dd833096b986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/home/jovyan/work/data/kmrd-small/countries.csv\")\n",
    "countries.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22fbb261-63df-4079-9f97-4b4ac39397ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie: integer (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "genres = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/home/jovyan/work/data/kmrd-small/genres.csv\")\n",
    "genres.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2394ff00-b636-40f2-b070-15470d713559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: integer (nullable = true)\n",
      " |-- movie: integer (nullable = true)\n",
      " |-- rate: integer (nullable = true)\n",
      " |-- time: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rates = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/home/jovyan/work/data/kmrd-small/rates.csv\")\n",
    "rates.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "702bef2a-ea8f-4fce-acf5-98904807394d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'movies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m connString \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjdbc:mysql://mysql:3306/default\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m accessInfo \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscott\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtiger\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmovies\u001b[49m\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mjdbc(connString, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmovies\u001b[39m\u001b[38;5;124m\"\u001b[39m, properties\u001b[38;5;241m=\u001b[39maccessInfo)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'movies' is not defined"
     ]
    }
   ],
   "source": [
    "connString = \"jdbc:mysql://mysql:3306/default\"\n",
    "accessInfo = {\"user\":\"scott\", \"password\":\"tiger\"}\n",
    "movies.write.mode(\"overwrite\").jdbc(connString, \"movies\", properties=accessInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02b2c59f-d543-4deb-a2e6-afd34552bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "peoples.write.mode(\"overwrite\").jdbc(connString, \"peoples\", properties=accessInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49f9d2ef-7218-49fa-8a20-299d23bf090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "castings.write.mode(\"overwrite\").jdbc(connString, \"castings\", properties=accessInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e50ff7c3-d2fa-44e5-9412-c1a1ddefd952",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.write.mode(\"overwrite\").jdbc(connString, \"countries\", properties=accessInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d4da756-ef85-4d74-bb58-1e1aa2212f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres.write.mode(\"overwrite\").jdbc(connString, \"genres\", properties=accessInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4413e56-253a-4d5b-b0f7-96130be1b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_ts = rates.withColumn(\"timestamp\", from_unixtime(col(\"time\"), \"yyyy-MM-dd HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3341c6eb-5cd0-494c-b510-a91af5de37da",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_ts.write.mode(\"overwrite\").jdbc(connString, \"rates\", properties=accessInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bb6dbb-0dbc-4cc8-bbda-19a8790ecf9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
